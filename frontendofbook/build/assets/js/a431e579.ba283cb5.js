"use strict";(globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend=globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend||[]).push([[34],{3286(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"digital-twin/sensor-simulation-validation","title":"Sensor Simulation & Validation","description":"Introduction to Sensor Simulation in Robotics","source":"@site/docs/digital-twin/sensor-simulation-validation.md","sourceDirName":"digital-twin","slug":"/digital-twin/sensor-simulation-validation","permalink":"/docs/digital-twin/sensor-simulation-validation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/digital-twin/sensor-simulation-validation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Sensor Simulation & Validation"},"sidebar":"tutorialSidebar","previous":{"title":"Digital Twins & HRI in Unity","permalink":"/docs/digital-twin/digital-twins-hri-unity"},"next":{"title":"NVIDIA Isaac Sim for Photorealistic Simulation","permalink":"/docs/isaac-ai-brain/isaac-sim-photorealistic-simulation"}}');var s=i(4848),t=i(8453);const r={sidebar_position:3,title:"Sensor Simulation & Validation"},o="Sensor Simulation & Validation",l={},d=[{value:"Introduction to Sensor Simulation in Robotics",id:"introduction-to-sensor-simulation-in-robotics",level:2},{value:"The Importance of Sensor Simulation",id:"the-importance-of-sensor-simulation",level:3},{value:"Sensor Types for Humanoid Robotics",id:"sensor-types-for-humanoid-robotics",level:3},{value:"Simulation Fidelity Requirements",id:"simulation-fidelity-requirements",level:3},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"LiDAR Physics and Characteristics",id:"lidar-physics-and-characteristics",level:3},{value:"Gazebo LiDAR Plugin",id:"gazebo-lidar-plugin",level:3},{value:"3D LiDAR Configuration",id:"3d-lidar-configuration",level:3},{value:"Realistic LiDAR Noise Modeling",id:"realistic-lidar-noise-modeling",level:3},{value:"LiDAR Performance Optimization",id:"lidar-performance-optimization",level:3},{value:"Point Cloud Processing in Simulation",id:"point-cloud-processing-in-simulation",level:3},{value:"Noise Models for LiDAR",id:"noise-models-for-lidar",level:3},{value:"Point Cloud Generation",id:"point-cloud-generation",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Physics and Characteristics",id:"depth-camera-physics-and-characteristics",level:3},{value:"Advanced RGB-D Camera Configuration",id:"advanced-rgb-d-camera-configuration",level:3},{value:"Realistic Depth Camera Noise Modeling",id:"realistic-depth-camera-noise-modeling",level:3},{value:"Depth Camera Processing Pipeline",id:"depth-camera-processing-pipeline",level:3},{value:"Performance Considerations for Depth Cameras",id:"performance-considerations-for-depth-cameras",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Physics and Characteristics",id:"imu-physics-and-characteristics",level:3},{value:"Advanced IMU Configuration",id:"advanced-imu-configuration",level:3},{value:"Multiple IMU Configuration for Humanoid Robots",id:"multiple-imu-configuration-for-humanoid-robots",level:3},{value:"Advanced IMU Data Processing",id:"advanced-imu-data-processing",level:3},{value:"IMU Calibration Techniques",id:"imu-calibration-techniques",level:3},{value:"Sensor Fusion Techniques",id:"sensor-fusion-techniques",level:2},{value:"Kalman Filter for Sensor Fusion",id:"kalman-filter-for-sensor-fusion",level:3},{value:"Validation Methods",id:"validation-methods",level:2},{value:"Comparing Simulated vs. Real Data",id:"comparing-simulated-vs-real-data",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercises",id:"exercises-1",level:2},{value:"Exercise 1: LiDAR Sensor Configuration",id:"exercise-1-lidar-sensor-configuration",level:3},{value:"Exercise 2: Depth Camera Integration",id:"exercise-2-depth-camera-integration",level:3},{value:"Exercise 3: IMU Calibration and Fusion",id:"exercise-3-imu-calibration-and-fusion",level:3},{value:"Exercise 4: Multi-Sensor Integration",id:"exercise-4-multi-sensor-integration",level:3},{value:"Validation and Troubleshooting Tips",id:"validation-and-troubleshooting-tips",level:2},{value:"Sensor Simulation Validation Techniques",id:"sensor-simulation-validation-techniques",level:3},{value:"Cross-Validation with Real Sensors",id:"cross-validation-with-real-sensors",level:4},{value:"Common Validation Metrics",id:"common-validation-metrics",level:4},{value:"Simulation-to-Reality Gap Assessment",id:"simulation-to-reality-gap-assessment",level:4},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:3},{value:"LiDAR Simulation Issues",id:"lidar-simulation-issues",level:4},{value:"Depth Camera Issues",id:"depth-camera-issues",level:4},{value:"IMU Simulation Issues",id:"imu-simulation-issues",level:4},{value:"General Troubleshooting Tips",id:"general-troubleshooting-tips",level:4},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Sensor-Specific Optimizations",id:"sensor-specific-optimizations",level:4},{value:"Integration Optimizations",id:"integration-optimizations",level:4},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sensor-simulation--validation",children:"Sensor Simulation & Validation"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-sensor-simulation-in-robotics",children:"Introduction to Sensor Simulation in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers the simulation of various sensors commonly used in humanoid robotics, including LiDAR, depth cameras, and IMUs. Proper sensor simulation is crucial for developing and validating perception algorithms in a safe, controlled environment before deployment on real robots."}),"\n",(0,s.jsx)(n.h3,{id:"the-importance-of-sensor-simulation",children:"The Importance of Sensor Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Sensor simulation plays a critical role in robotics development for several key reasons:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safe Algorithm Development"}),": Test perception and navigation algorithms without risk to expensive hardware"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost-Effective Training"}),": Train machine learning models on large datasets of simulated sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Controlled Testing"}),": Create reproducible test scenarios with known ground truth"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge Case Exploration"}),": Generate rare or dangerous scenarios safely in simulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware-in-the-Loop Testing"}),": Validate sensor fusion algorithms before deployment"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots specifically, sensor simulation is essential because these robots operate in human environments where safety is paramount and physical testing can be extremely costly."}),"\n",(0,s.jsx)(n.h3,{id:"sensor-types-for-humanoid-robotics",children:"Sensor Types for Humanoid Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots typically use a combination of the following sensor types:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR"}),": Provides 2D or 3D range data for mapping and navigation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Cameras"}),": RGB-D sensors for environment perception and object recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMUs"}),": Inertial measurement units for balance, orientation, and motion detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Located in joints or feet for contact detection and balance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tactile Sensors"}),": For fine manipulation and contact feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"}),": Visual information for object recognition and scene understanding"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"simulation-fidelity-requirements",children:"Simulation Fidelity Requirements"}),"\n",(0,s.jsx)(n.p,{children:"The fidelity required for sensor simulation depends on the intended application:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Algorithm Development"}),": High fidelity to ensure transferability to real robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training Data Generation"}),": Sufficient realism to be useful for machine learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Validation"}),": Accurate modeling of sensor limitations and failure modes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Evaluation"}),": Realistic noise models and response characteristics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand LiDAR simulation with realistic noise models"}),"\n",(0,s.jsx)(n.li,{children:"Learn about depth camera simulation and point cloud generation"}),"\n",(0,s.jsx)(n.li,{children:"Master IMU simulation with realistic drift and noise characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Explore sensor fusion techniques in simulation environments"}),"\n",(0,s.jsx)(n.li,{children:"Validate sensor simulation accuracy against real-world data"}),"\n",(0,s.jsx)(n.li,{children:"Compare simulated vs. real sensor data for algorithm validation"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion algorithms for improved state estimation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of basic sensor types and their applications"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of physics simulation from Chapter 1"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with digital twin concepts from Chapter 2"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Light Detection and Ranging (LiDAR) sensors are crucial for humanoid robots, providing accurate 2D or 3D spatial information for navigation, mapping, and obstacle detection."}),"\n",(0,s.jsx)(n.h3,{id:"lidar-physics-and-characteristics",children:"LiDAR Physics and Characteristics"}),"\n",(0,s.jsx)(n.p,{children:"LiDAR sensors work by emitting laser pulses and measuring the time it takes for the light to return after reflecting off objects. Key characteristics include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Range"}),": Distance measurement capability (typically 0.1m to 30m for robotics)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Field of View"}),": Angular coverage (horizontal and vertical)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution"}),": Angular resolution and distance accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update Rate"}),": How frequently measurements are taken (typically 5-20 Hz)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy"}),": Measurement precision and repeatability"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"gazebo-lidar-plugin",children:"Gazebo LiDAR Plugin"}),"\n",(0,s.jsx)(n.p,{children:"Gazebo provides a robust LiDAR sensor plugin that simulates 2D and 3D LiDAR sensors with realistic characteristics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="lidar_mount">\n  <sensor type="ray" name="humanoid_lidar_2d">\n    <pose>0 0 0.5 0 0 0</pose> \x3c!-- Mount at 0.5m height --\x3e\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples> \x3c!-- 0.5 degree resolution over 360 degrees --\x3e\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle> \x3c!-- -\u03c0 radians (-180 degrees) --\x3e\n          <max_angle>3.14159</max_angle>  \x3c!-- \u03c0 radians (180 degrees) --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min> \x3c!-- Minimum detectable range: 0.1m --\x3e\n        <max>30.0</max> \x3c!-- Maximum detectable range: 30m --\x3e\n        <resolution>0.01</resolution> \x3c!-- Range resolution: 1cm --\x3e\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_laser.so">\n      <topicName>/humanoid_robot/laser_scan</topicName>\n      <frameName>lidar_mount</frameName>\n      <min_range>0.1</min_range>\n      <max_range>30.0</max_range>\n      <gaussian_noise>0.01</gaussian_noise> \x3c!-- 1cm noise standard deviation --\x3e\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3d-lidar-configuration",children:"3D LiDAR Configuration"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots requiring full 3D perception, a 3D LiDAR configuration is essential:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="lidar_mount">\n  <sensor type="gpu_ray" name="humanoid_lidar_3d">\n    <pose>0 0 0.5 0 0 0</pose>\n    <visualize>false</visualize> \x3c!-- Disable visualization for performance --\x3e\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>640</samples> \x3c!-- Horizontal samples --\x3e\n          <resolution>1</resolution>\n          <min_angle>-1.3962634</min_angle> \x3c!-- -80 degrees --\x3e\n          <max_angle>1.3962634</max_angle>  \x3c!-- 80 degrees --\x3e\n        </horizontal>\n        <vertical>\n          <samples>32</samples> \x3c!-- Vertical channels --\x3e\n          <resolution>1</resolution>\n          <min_angle>-0.261799387</min_angle> \x3c!-- -15 degrees --\x3e\n          <max_angle>0.261799387</max_angle>  \x3c!-- 15 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.08</min> \x3c!-- Minimum range --\x3e\n        <max>100.0</max> \x3c!-- Maximum range --\x3e\n        <resolution>0.01</resolution> \x3c!-- Range resolution --\x3e\n      </range>\n    </ray>\n    <plugin name="velodyne_controller" filename="libgazebo_ros_velodyne_gpu_laser.so">\n      <topicName>/humanoid_robot/velodyne_points</topicName>\n      <frameName>lidar_mount</frameName>\n      <min_range>0.9</min_range>\n      <max_range>100.0</max_range>\n      <gaussian_noise>0.01</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"realistic-lidar-noise-modeling",children:"Realistic LiDAR Noise Modeling"}),"\n",(0,s.jsx)(n.p,{children:"Realistic noise modeling is crucial for transfer learning from simulation to reality:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gaussian Noise"}),": Random measurement errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bias"}),": Systematic measurement offsets"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Drift"}),": Time-varying systematic errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dropout"}),": Occasional complete measurement failures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-path Interference"}),": Reflections causing false readings"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Advanced noise modeling for realistic LiDAR simulation --\x3e\n<sensor type="ray" name="advanced_lidar">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1080</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>25.0</max>\n      <resolution>0.001</resolution>\n    </range>\n  </ray>\n  <plugin name="advanced_lidar_plugin" filename="libgazebo_ros_laser.so">\n    <topicName>/humanoid_robot/advanced_laser_scan</topicName>\n    <frameName>lidar_mount</frameName>\n    \x3c!-- Noise parameters --\x3e\n    <gaussian_noise>0.008</gaussian_noise> \x3c!-- 8mm noise --\x3e\n    <bias_mean>0.002</bias_mean> \x3c!-- 2mm bias --\x3e\n    <bias_stddev>0.001</bias_stddev> \x3c!-- 1mm bias variation --\x3e\n    \x3c!-- Range-dependent noise --\x3e\n    <return_phase_correction>0.0</return_phase_correction>\n    <signal_size>0.0</signal_size>\n    <reference_coordinates>0</reference_coordinates>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"lidar-performance-optimization",children:"LiDAR Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots with limited computational resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reduced Resolution"}),": Lower sample counts for faster processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Limited FOV"}),": Focus on relevant directions only"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Variable Update Rate"}),": Adjust based on robot speed and environment complexity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Selective Visualization"}),": Only visualize when debugging"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"point-cloud-processing-in-simulation",children:"Point Cloud Processing in Simulation"}),"\n",(0,s.jsx)(n.p,{children:"3D LiDAR sensors generate point clouds that require specialized processing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport numpy as np\nimport rospy\nfrom sensor_msgs.msg import PointCloud2\nimport sensor_msgs.point_cloud2 as pc2\nfrom std_msgs.msg import Header\nfrom geometry_msgs.msg import Point32\n\nclass LiDARProcessor:\n    def __init__(self):\n        rospy.init_node(\'lidar_processor\')\n        self.subscriber = rospy.Subscriber(\'/humanoid_robot/velodyne_points\',\n                                          PointCloud2, self.point_cloud_callback)\n        self.ground_publisher = rospy.Publisher(\'/humanoid_robot/ground_points\',\n                                               PointCloud2, queue_size=1)\n        self.obstacle_publisher = rospy.Publisher(\'/humanoid_robot/obstacle_points\',\n                                                 PointCloud2, queue_size=1)\n\n    def point_cloud_callback(self, msg):\n        """\n        Process incoming point cloud data from simulated LiDAR\n        """\n        # Convert PointCloud2 to list of points\n        points = list(pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True))\n\n        # Convert to numpy array for processing\n        points_array = np.array(points)\n\n        if len(points_array) == 0:\n            return\n\n        # Segment ground plane using RANSAC\n        ground_points, obstacle_points = self.segment_ground_plane(points_array)\n\n        # Publish segmented point clouds\n        self.publish_point_cloud(ground_points, self.ground_publisher, msg.header)\n        self.publish_point_cloud(obstacle_points, self.obstacle_publisher, msg.header)\n\n    def segment_ground_plane(self, points, distance_threshold=0.1, max_iterations=100):\n        """\n        Segment ground plane from point cloud using RANSAC\n        """\n        if len(points) < 100:  # Need minimum points for RANSAC\n            return np.array([]), points\n\n        best_model = None\n        best_inliers = []\n\n        for _ in range(max_iterations):\n            # Randomly sample 3 points\n            sample_indices = np.random.choice(len(points), 3, replace=False)\n            sample_points = points[sample_indices]\n\n            # Fit plane to 3 points\n            plane_model = self.fit_plane(sample_points)\n\n            if plane_model is None:\n                continue\n\n            # Find inliers\n            distances = np.abs(np.dot(points - sample_points[0], plane_model[:3]) + plane_model[3])\n            inliers = points[distances < distance_threshold]\n\n            if len(inliers) > len(best_inliers):\n                best_inliers = inliers\n                best_model = plane_model\n\n        if best_model is not None:\n            # Separate ground and obstacles\n            ground_mask = np.abs(np.dot(points - best_inliers[0], best_model[:3]) + best_model[3]) < distance_threshold\n            ground_points = points[ground_mask]\n            obstacle_points = points[~ground_mask]\n            return ground_points, obstacle_points\n        else:\n            # If RANSAC failed, return original points\n            return np.array([]), points\n\n    def fit_plane(self, points):\n        """\n        Fit a plane to 3 points: ax + by + cz + d = 0\n        """\n        if len(points) < 3:\n            return None\n\n        p1, p2, p3 = points[:3]\n\n        # Calculate plane normal vector\n        v1 = p2 - p1\n        v2 = p3 - p1\n        normal = np.cross(v1, v2)\n\n        if np.linalg.norm(normal) < 1e-6:  # Points are collinear\n            return None\n\n        normal = normal / np.linalg.norm(normal)\n        d = -np.dot(normal, p1)\n\n        return np.append(normal, d)\n\n    def publish_point_cloud(self, points, publisher, header):\n        """\n        Publish point cloud to ROS topic\n        """\n        if len(points) == 0:\n            return\n\n        # Create PointCloud2 message\n        header = Header()\n        header.stamp = rospy.Time.now()\n        header.frame_id = "lidar_frame"\n\n        # Convert numpy array to PointCloud2\n        cloud_points = [Point32(x, y, z) for x, y, z in points]\n        cloud_msg = pc2.create_cloud_xyz32(header, cloud_points)\n\n        publisher.publish(cloud_msg)\n\nif __name__ == \'__main__\':\n    processor = LiDARProcessor()\n    rospy.spin()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"noise-models-for-lidar",children:"Noise Models for LiDAR"}),"\n",(0,s.jsx)(n.p,{children:"Realistic LiDAR simulation includes various noise sources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gaussian Noise"}),": Random measurement errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bias"}),": Systematic measurement offsets"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Drift"}),": Time-varying systematic errors"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"point-cloud-generation",children:"Point Cloud Generation"}),"\n",(0,s.jsx)(n.p,{children:"3D LiDAR sensors generate point clouds that require special handling:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor type="gpu_ray" name="3d_lidar">\n  <pose>0 0 0.1 0 0 0</pose>\n  <visualize>false</visualize>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>640</samples>\n        <resolution>1</resolution>\n        <min_angle>-1.3962634</min_angle>\n        <max_angle>1.3962634</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>32</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.261799387</min_angle>\n        <max_angle>0.261799387</max_angle>\n      </vertical>\n    </scan>\n    <range>\n      <min>0.08</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,s.jsx)(n.p,{children:"RGB-D cameras (color + depth) are essential sensors for humanoid robots, providing both visual information and depth data for object recognition, scene understanding, and navigation."}),"\n",(0,s.jsx)(n.h3,{id:"depth-camera-physics-and-characteristics",children:"Depth Camera Physics and Characteristics"}),"\n",(0,s.jsx)(n.p,{children:"Depth cameras work by measuring the distance to objects in the scene using various technologies:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Vision"}),": Uses two cameras to calculate depth from parallax"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structured Light"}),": Projects a known pattern and measures distortions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time-of-Flight"}),": Measures the time light takes to return from projected pulses"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Key characteristics include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution"}),": Image dimensions (e.g., 640x480, 1280x720)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Field of View"}),": Angular coverage (horizontal and vertical)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Range"}),": Minimum and maximum measurable distances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Accuracy"}),": Precision of distance measurements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update Rate"}),": Frame rate (typically 15-30 Hz)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"advanced-rgb-d-camera-configuration",children:"Advanced RGB-D Camera Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="camera_mount">\n  <sensor type="depth" name="humanoid_depth_camera_advanced">\n    <always_on>true</always_on>\n    <visualize>true</visualize>\n    <update_rate>30.0</update_rate>\n    <camera name="head_camera">\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n      <image>\n        <format>R8G8B8</format>\n        <width>1280</width> \x3c!-- Higher resolution for detail --\x3e\n        <height>720</height>\n      </image>\n      <clip>\n        <near>0.1</near> \x3c!-- Minimum range: 10cm --\x3e\n        <far>8.0</far>   \x3c!-- Maximum range: 8m --\x3e\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev> \x3c!-- 7mm depth noise --\x3e\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      \x3c!-- Camera parameters --\x3e\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>head_camera</cameraName>\n\n      \x3c!-- Topic names --\x3e\n      <imageTopicName>rgb/image_raw</imageTopicName>\n      <depthImageTopicName>depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n\n      \x3c!-- Frame configuration --\x3e\n      <frameName>camera_depth_optical_frame</frameName>\n      <pointCloudCutoff>0.1</pointCloudCutoff>    \x3c!-- Near cutoff --\x3e\n      <pointCloudCutoffMax>8.0</pointCloudCutoffMax> \x3c!-- Far cutoff --\x3e\n\n      \x3c!-- Distortion parameters --\x3e\n      <distortion_k1>0.001</distortion_k1>\n      <distortion_k2>0.0005</distortion_k2>\n      <distortion_k3>0.0001</distortion_k3>\n      <distortion_t1>0.0002</distortion_t1>\n      <distortion_t2>0.0003</distortion_t2>\n\n      \x3c!-- Intrinsic parameters --\x3e\n      <CxPrime>0.0</CxPrime>\n      <Cx>640.0</Cx> \x3c!-- Principal point X --\x3e\n      <Cy>360.0</Cy> \x3c!-- Principal point Y --\x3e\n      <focalLength>640.0</focalLength> \x3c!-- Focal length in pixels --\x3e\n      <hackBaseline>0.0</hackBaseline>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"realistic-depth-camera-noise-modeling",children:"Realistic Depth Camera Noise Modeling"}),"\n",(0,s.jsx)(n.p,{children:"Realistic noise modeling is critical for depth cameras:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Depth camera with realistic noise model --\x3e\n<sensor type="depth" name="realistic_depth_camera">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n    \x3c!-- Color channel noise --\x3e\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.005</stddev>\n    </noise>\n  </camera>\n  <plugin name="realistic_camera_plugin" filename="libgazebo_ros_openni_kinect.so">\n    <cameraName>realistic_camera</cameraName>\n    \x3c!-- Advanced noise parameters --\x3e\n    <depthImageTopicName>depth/image_raw_noisy</depthImageTopicName>\n    \x3c!-- Depth-dependent noise --\x3e\n    <pointCloudCutoff>0.3</pointCloudCutoff>\n    <pointCloudCutoffMax>5.0</pointCloudCutoffMax>\n    \x3c!-- Add more realistic distortion --\x3e\n    <distortion_k1>0.0012</distortion_k1>\n    <distortion_k2>-0.0003</distortion_k2>\n    <distortion_k3>0.0001</distortion_k3>\n    <distortion_t1>0.0001</distortion_t1>\n    <distortion_t2>-0.0002</distortion_t2>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"depth-camera-processing-pipeline",children:"Depth Camera Processing Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport numpy as np\nimport rospy\nimport cv2\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Header\nimport sensor_msgs.point_cloud2 as pc2\nfrom tf.transformations import quaternion_from_euler\nimport message_filters\n\nclass DepthCameraProcessor:\n    def __init__(self):\n        rospy.init_node(\'depth_camera_processor\')\n\n        # Initialize bridge for image conversion\n        self.bridge = CvBridge()\n\n        # Synchronize RGB and depth images\n        rgb_sub = message_filters.Subscriber(\'/humanoid_robot/rgb/image_raw\', Image)\n        depth_sub = message_filters.Subscriber(\'/humanoid_robot/depth/image_raw\', Image)\n\n        ts = message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 0.1)\n        ts.registerCallback(self.synchronized_callback)\n\n        # Publishers for processed data\n        self.object_detection_pub = rospy.Publisher(\'/humanoid_robot/object_detections\',\n                                                   PointCloud2, queue_size=1)\n        self.surface_normal_pub = rospy.Publisher(\'/humanoid_robot/surface_normals\',\n                                                 PointCloud2, queue_size=1)\n\n        # Camera intrinsics (will be updated from camera_info)\n        self.fx = 640.0  # Focal length x\n        self.fy = 640.0  # Focal length y\n        self.cx = 640.0  # Principal point x\n        self.cy = 360.0  # Principal point y\n\n    def synchronized_callback(self, rgb_msg, depth_msg):\n        """\n        Process synchronized RGB and depth images\n        """\n        try:\n            # Convert ROS images to OpenCV\n            rgb_image = self.bridge.imgmsg_to_cv2(rgb_msg, "bgr8")\n            depth_image = self.bridge.imgmsg_to_cv2(depth_msg, "32FC1")\n\n            # Process the images\n            object_points = self.detect_objects(rgb_image, depth_image)\n            surface_normals = self.calculate_surface_normals(depth_image)\n\n            # Publish results\n            if object_points.size > 0:\n                self.publish_point_cloud(object_points, self.object_detection_pub, rgb_msg.header)\n\n            if surface_normals.size > 0:\n                self.publish_point_cloud(surface_normals, self.surface_normal_pub, rgb_msg.header)\n\n        except Exception as e:\n            rospy.logerr(f"Error processing synchronized images: {e}")\n\n    def detect_objects(self, rgb_image, depth_image):\n        """\n        Simple object detection using color and depth information\n        """\n        # Convert BGR to HSV for better color detection\n        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for object detection (example: red objects)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n\n        # Apply depth mask to focus on objects at certain distances\n        depth_mask = (depth_image > 0.5) & (depth_image < 3.0)  # 50cm to 3m\n        combined_mask = mask & depth_mask\n\n        # Find contours\n        contours, _ = cv2.findContours(combined_mask.astype(np.uint8),\n                                      cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        object_points = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                # Get the center of the contour\n                M = cv2.moments(contour)\n                if M["m00"] != 0:\n                    cX = int(M["m10"] / M["m00"])\n                    cY = int(M["m01"] / M["m00"])\n\n                    # Get depth at this point\n                    depth_val = depth_image[cY, cX]\n                    if depth_val > 0:  # Valid depth\n                        # Convert to 3D point\n                        x = (cX - self.cx) * depth_val / self.fx\n                        y = (cY - self.cy) * depth_val / self.fy\n                        z = depth_val\n\n                        object_points.append([x, y, z])\n\n        return np.array(object_points)\n\n    def calculate_surface_normals(self, depth_image):\n        """\n        Calculate surface normals from depth image\n        """\n        # Calculate gradients\n        zx = cv2.Sobel(depth_image, cv2.CV_64F, 1, 0, ksize=3)\n        zy = cv2.Sobel(depth_image, cv2.CV_64F, 0, 1, ksize=3)\n\n        # Calculate surface normals\n        # For depth image, normal = (-dz/dx, -dz/dy, 1)\n        normals = np.zeros((depth_image.shape[0], depth_image.shape[1], 3))\n        normals[:, :, 0] = -zx  # -dz/dx\n        normals[:, :, 1] = -zy  # -dz/dy\n        normals[:, :, 2] = 1.0  # dz/dz = 1\n\n        # Normalize\n        norm = np.linalg.norm(normals, axis=2, keepdims=True)\n        norm = np.where(norm == 0, 1, norm)  # Avoid division by zero\n        normals = normals / norm\n\n        # Sample normals for visualization\n        sample_step = 20\n        sampled_normals = []\n\n        for y in range(0, depth_image.shape[0], sample_step):\n            for x in range(0, depth_image.shape[1], sample_step):\n                if depth_image[y, x] > 0:  # Valid depth\n                    # Convert to 3D point\n                    point_x = (x - self.cx) * depth_image[y, x] / self.fx\n                    point_y = (y - self.cy) * depth_image[y, x] / self.fy\n                    point_z = depth_image[y, x]\n\n                    # Add normal vector\n                    normal_x = normals[y, x, 0]\n                    normal_y = normals[y, x, 1]\n                    normal_z = normals[y, x, 2]\n\n                    sampled_normals.extend([\n                        [point_x, point_y, point_z],  # Point\n                        [point_x + normal_x * 0.1, point_y + normal_y * 0.1, point_z + normal_z * 0.1]  # Normal endpoint\n                    ])\n\n        return np.array(sampled_normals)\n\n    def publish_point_cloud(self, points, publisher, header):\n        """\n        Publish point cloud to ROS topic\n        """\n        if len(points) == 0:\n            return\n\n        # Create PointCloud2 message\n        header = Header()\n        header.stamp = rospy.Time.now()\n        header.frame_id = "camera_depth_optical_frame"\n\n        # Convert numpy array to PointCloud2\n        from geometry_msgs.msg import Point32\n        cloud_points = [Point32(x, y, z) for x, y, z in points]\n        cloud_msg = pc2.create_cloud_xyz32(header, cloud_points)\n\n        publisher.publish(cloud_msg)\n\nif __name__ == \'__main__\':\n    processor = DepthCameraProcessor()\n    rospy.spin()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"performance-considerations-for-depth-cameras",children:"Performance Considerations for Depth Cameras"}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots with computational constraints:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution Selection"}),": Balance detail vs. processing speed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROI Processing"}),": Focus computation on relevant regions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal Filtering"}),": Use previous frames to improve current estimates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Downsampling"}),": Reduce point cloud density where precision isn't critical"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-scale Processing"}),": Use different resolutions for different tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Inertial Measurement Units (IMUs) are critical sensors for humanoid robots, providing information about orientation, angular velocity, and linear acceleration. This data is essential for balance control, motion detection, and navigation."}),"\n",(0,s.jsx)(n.h3,{id:"imu-physics-and-characteristics",children:"IMU Physics and Characteristics"}),"\n",(0,s.jsx)(n.p,{children:"IMUs typically contain three types of sensors:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gyroscopes"}),": Measure angular velocity (rate of rotation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accelerometers"}),": Measure linear acceleration (including gravity)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Magnetometers"}),": Measure magnetic field (for heading reference)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Key characteristics include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Measurement Range"}),": Maximum measurable values (e.g., \xb12000\xb0/s for gyros)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Density"}),": Noise per square root of bandwidth (e.g., 4 mdps/\u221aHz)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bias Stability"}),": How much the bias drifts over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update Rate"}),": How frequently measurements are available (typically 100-1000 Hz)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Factor Error"}),": Deviation from ideal measurement scale"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-Axis Sensitivity"}),": Response to inputs in non-sensitive axes"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"advanced-imu-configuration",children:"Advanced IMU Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor type="imu" name="humanoid_imu_advanced">\n    <always_on>true</always_on>\n    <update_rate>200</update_rate> \x3c!-- Higher update rate for humanoid balance --\x3e\n    <visualize>false</visualize>\n    <topic>__default_topic</topic>\n    <imu>\n      \x3c!-- Angular velocity measurements --\x3e\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.047e-3</stddev>  \x3c!-- ~0.06 deg/s (low noise IMU) --\x3e\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>8.727e-5</bias_stddev>  \x3c!-- ~0.005 deg/s bias stability --\x3e\n            <dynamic_bias_std>8.727e-5</dynamic_bias_std>  \x3c!-- Bias walk --\x3e\n            <dynamic_bias_correlation_time>100.0</dynamic_bias_correlation_time>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.047e-3</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>8.727e-5</bias_stddev>\n            <dynamic_bias_std>8.727e-5</dynamic_bias_std>\n            <dynamic_bias_correlation_time>100.0</dynamic_bias_correlation_time>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.047e-3</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>8.727e-5</bias_stddev>\n            <dynamic_bias_std>8.727e-5</dynamic_bias_std>\n            <dynamic_bias_correlation_time>100.0</dynamic_bias_correlation_time>\n          </noise>\n        </z>\n      </angular_velocity>\n\n      \x3c!-- Linear acceleration measurements --\x3e\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.0e-2</stddev>  \x3c!-- 10 mg noise --\x3e\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>1.0e-3</bias_stddev>  \x3c!-- 1 mg bias --\x3e\n            <dynamic_bias_std>1.0e-4</dynamic_bias_std>  \x3c!-- Bias walk --\x3e\n            <dynamic_bias_correlation_time>300.0</dynamic_bias_correlation_time>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.0e-2</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>1.0e-3</bias_stddev>\n            <dynamic_bias_std>1.0e-4</dynamic_bias_std>\n            <dynamic_bias_correlation_time>300.0</dynamic_bias_correlation_time>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.0e-2</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>1.0e-3</bias_stddev>\n            <dynamic_bias_std>1.0e-4</dynamic_bias_std>\n            <dynamic_bias_correlation_time>300.0</dynamic_bias_correlation_time>\n          </noise>\n        </z>\n      </linear_acceleration>\n\n      \x3c!-- Angular velocity saturation limits --\x3e\n      <angular_velocity>\n        <x>\n          <saturation_level>3.49065850399</saturation_level> \x3c!-- 200 deg/s --\x3e\n        </x>\n        <y>\n          <saturation_level>3.49065850399</saturation_level> \x3c!-- 200 deg/s --\x3e\n        </y>\n        <z>\n          <saturation_level>3.49065850399</saturation_level> \x3c!-- 200 deg/s --\x3e\n        </z>\n      </angular_velocity>\n\n      \x3c!-- Linear acceleration saturation limits --\x3e\n      <linear_acceleration>\n        <x>\n          <saturation_level>196.2</saturation_level> \x3c!-- 20g --\x3e\n        </x>\n        <y>\n          <saturation_level>196.2</saturation_level> \x3c!-- 20g --\x3e\n        </y>\n        <z>\n          <saturation_level>196.2</saturation_level> \x3c!-- 20g --\x3e\n        </z>\n      </linear_acceleration>\n    </imu>\n\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n      <topicName>/humanoid_robot/imu/data</topicName>\n      <serviceName>/humanoid_robot/imu/service</serviceName>\n      <bodyName>imu_link</bodyName>\n      <frameName>imu_link</frameName>\n      <updateRate>200.0</updateRate>\n      <gaussianNoise>1.0e-2</gaussianNoise>\n      <xyzOffset>0 0 0</xyzOffset>\n      <rpyOffset>0 0 0</rpyOffset>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multiple-imu-configuration-for-humanoid-robots",children:"Multiple IMU Configuration for Humanoid Robots"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots often have multiple IMUs for distributed sensing:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU in the head for orientation estimation --\x3e\n<gazebo reference="head_imu_link">\n  <sensor type="imu" name="head_imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x><noise type="gaussian"><stddev>1.222e-3</stddev></noise></x>\n        <y><noise type="gaussian"><stddev>1.222e-3</stddev></noise></y>\n        <z><noise type="gaussian"><stddev>1.222e-3</stddev></noise></z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x><noise type="gaussian"><stddev>1.7e-2</stddev></noise></x>\n        <y><noise type="gaussian"><stddev>1.7e-2</stddev></noise></y>\n        <z><noise type="gaussian"><stddev>1.7e-2</stddev></noise></z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="head_imu_plugin" filename="libgazebo_ros_imu.so">\n      <topicName>/humanoid_robot/head_imu</topicName>\n      <frameName>head_imu_link</frameName>\n      <updateRate>100.0</updateRate>\n    </plugin>\n  </sensor>\n</gazebo>\n\n\x3c!-- IMU in the torso for balance control --\x3e\n<gazebo reference="torso_imu_link">\n  <sensor type="imu" name="torso_imu">\n    <always_on>true</always_on>\n    <update_rate>200</update_rate>\n    <imu>\n      <angular_velocity>\n        <x><noise type="gaussian"><stddev>8.73e-4</stddev></noise></x>\n        <y><noise type="gaussian"><stddev>8.73e-4</stddev></noise></y>\n        <z><noise type="gaussian"><stddev>8.73e-4</stddev></noise></z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x><noise type="gaussian"><stddev>1.0e-2</stddev></noise></x>\n        <y><noise type="gaussian"><stddev>1.0e-2</stddev></noise></y>\n        <z><noise type="gaussian"><stddev>1.0e-2</stddev></noise></z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="torso_imu_plugin" filename="libgazebo_ros_imu.so">\n      <topicName>/humanoid_robot/torso_imu</topicName>\n      <frameName>torso_imu_link</frameName>\n      <updateRate>200.0</updateRate>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"advanced-imu-data-processing",children:"Advanced IMU Data Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport numpy as np\nimport rospy\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Header\nfrom tf.transformations import quaternion_from_euler, euler_from_quaternion\nfrom scipy.spatial.transform import Rotation as R\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\nclass AdvancedIMUProcessor:\n    def __init__(self):\n        rospy.init_node(\'advanced_imu_processor\')\n\n        # IMU subscribers (for multiple IMUs)\n        self.head_imu_sub = rospy.Subscriber(\'/humanoid_robot/head_imu\',\n                                            Imu, self.head_imu_callback)\n        self.torso_imu_sub = rospy.Subscriber(\'/humanoid_robot/torso_imu\',\n                                             Imu, self.torso_imu_callback)\n\n        # Publishers\n        self.orientation_pub = rospy.Publisher(\'/humanoid_robot/fused_orientation\',\n                                              Imu, queue_size=1)\n        self.balance_pub = rospy.Publisher(\'/humanoid_robot/balance_state\',\n                                          Vector3, queue_size=1)\n\n        # IMU state variables\n        self.head_imu_data = None\n        self.torso_imu_data = None\n        self.last_head_time = None\n        self.last_torso_time = None\n\n        # Sensor fusion parameters\n        self.gyro_bias = np.zeros(3)  # Gyro bias estimate\n        self.orientation = R.from_quat([0, 0, 0, 1])  # Current orientation\n        self.gravity = np.array([0, 0, -9.81])  # Gravity vector\n        self.bias_stability = 1e-4  # Bias stability parameter\n\n        # Complementary filter parameters\n        self.acceleration_alpha = 0.01  # Low-pass filter coefficient for accelerometer\n        self.magnetic_alpha = 0.01    # Low-pass filter coefficient for magnetometer\n\n        # Data buffers for noise analysis\n        self.gyro_buffer = deque(maxlen=1000)\n        self.accel_buffer = deque(maxlen=1000)\n\n        # Bias estimation parameters\n        self.bias_learning_rate = 0.001\n        self.stationary_threshold = 0.1  # Threshold for stationary detection\n\n    def head_imu_callback(self, msg):\n        """\n        Process head IMU data\n        """\n        self.process_imu_data(msg, \'head\')\n\n    def torso_imu_callback(self, msg):\n        """\n        Process torso IMU data\n        """\n        self.process_imu_data(msg, \'torso\')\n\n    def process_imu_data(self, msg, sensor_name):\n        """\n        Process IMU data with advanced filtering\n        """\n        # Extract measurements\n        angular_velocity = np.array([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n\n        linear_acceleration = np.array([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ])\n\n        # Update data buffers for noise analysis\n        self.gyro_buffer.append(angular_velocity)\n        self.accel_buffer.append(linear_acceleration)\n\n        # Estimate bias if robot is likely stationary\n        if self.is_stationary(angular_velocity, linear_acceleration):\n            self.estimate_bias(angular_velocity)\n\n        # Remove bias from angular velocity\n        corrected_angular_velocity = angular_velocity - self.gyro_bias\n\n        # Calculate time delta\n        current_time = msg.header.stamp.to_sec()\n        if hasattr(self, f\'last_{sensor_name}_time\') and getattr(self, f\'last_{sensor_name}_time\') is not None:\n            dt = current_time - getattr(self, f\'last_{sensor_name}_time\')\n        else:\n            dt = 1.0/200.0  # Default to 200Hz if first measurement\n\n        # Update time\n        setattr(self, f\'last_{sensor_name}_time\', current_time)\n\n        # Perform sensor fusion\n        orientation_estimate = self.fuse_imu_data(\n            corrected_angular_velocity,\n            linear_acceleration,\n            dt\n        )\n\n        # Publish processed data\n        self.publish_processed_data(orientation_estimate, msg.header)\n\n    def is_stationary(self, angular_velocity, linear_acceleration):\n        """\n        Determine if the robot is stationary based on IMU readings\n        """\n        # Check if angular velocity is low\n        angular_magnitude = np.linalg.norm(angular_velocity)\n        linear_magnitude = np.linalg.norm(linear_acceleration - self.gravity)\n\n        return (angular_magnitude < self.stationary_threshold and\n                linear_magnitude < self.stationary_threshold)\n\n    def estimate_bias(self, angular_velocity):\n        """\n        Estimate and update gyro bias\n        """\n        # Simple bias estimation using exponential moving average\n        self.gyro_bias = (1 - self.bias_learning_rate) * self.gyro_bias + \\\n                         self.bias_learning_rate * angular_velocity\n\n    def fuse_imu_data(self, angular_velocity, linear_acceleration, dt):\n        """\n        Fuse gyroscope and accelerometer data using complementary filter\n        """\n        # Integrate gyroscope data for orientation\n        rotation_vector = angular_velocity * dt\n        delta_rotation = R.from_rotvec(rotation_vector)\n        new_orientation = delta_rotation * self.orientation\n\n        # Estimate orientation from accelerometer\n        if np.linalg.norm(linear_acceleration) > 0.1:  # Avoid division by zero\n            # Normalize accelerometer reading\n            accel_normalized = linear_acceleration / np.linalg.norm(linear_acceleration)\n            # Estimate gravity direction\n            gravity_direction = -accel_normalized\n\n            # Calculate roll and pitch from gravity vector\n            pitch = np.arctan2(-gravity_direction[0],\n                              np.sqrt(gravity_direction[1]**2 + gravity_direction[2]**2))\n            roll = np.arctan2(gravity_direction[1], gravity_direction[2])\n\n            # Create rotation from gravity (only roll and pitch)\n            accel_orientation = R.from_euler(\'xyz\', [roll, pitch, 0])\n\n            # Apply complementary filter\n            # Use accelerometer for low-frequency orientation (roll, pitch)\n            # Use gyroscope for high-frequency updates\n            current_euler = self.orientation.as_euler(\'xyz\')\n            accel_euler = accel_orientation.as_euler(\'xyz\')\n\n            # Complementary filter: blend accelerometer and gyroscope estimates\n            filtered_roll = (1 - self.acceleration_alpha) * current_euler[0] + \\\n                           self.acceleration_alpha * accel_euler[0]\n            filtered_pitch = (1 - self.acceleration_alpha) * current_euler[1] + \\\n                            self.acceleration_alpha * accel_euler[1]\n            filtered_yaw = current_euler[2]  # Yaw from gyroscope integration\n\n            gravity_based_orientation = R.from_euler(\'xyz\', [filtered_roll, filtered_pitch, filtered_yaw])\n\n            # Blend the two orientations\n            self.orientation = R.slerp([self.orientation, new_orientation], [0.9, 0.1])[0]\n            self.orientation = R.slerp([self.orientation, gravity_based_orientation], [0.7, 0.3])[0]\n        else:\n            # If accelerometer is unreliable, just use gyroscope\n            self.orientation = new_orientation\n\n        return self.orientation\n\n    def publish_processed_data(self, orientation, header):\n        """\n        Publish processed IMU data\n        """\n        # Create IMU message\n        imu_msg = Imu()\n        imu_msg.header = header\n        imu_msg.header.frame_id = "base_link"\n\n        # Set orientation\n        quat = orientation.as_quat()\n        imu_msg.orientation.x = quat[0]\n        imu_msg.orientation.y = quat[1]\n        imu_msg.orientation.z = quat[2]\n        imu_msg.orientation.w = quat[3]\n\n        # Set orientation covariance (diagonal of covariance matrix)\n        # 0.01 rad = ~0.57 degrees uncertainty\n        imu_msg.orientation_covariance = [0.01, 0, 0, 0, 0.01, 0, 0, 0, 0.01]\n\n        # Calculate balance metrics\n        euler = orientation.as_euler(\'xyz\')\n        balance_vector = Vector3()\n        balance_vector.x = euler[0]  # Roll\n        balance_vector.y = euler[1]  # Pitch\n        balance_vector.z = euler[2]  # Yaw\n\n        # Publish data\n        self.orientation_pub.publish(imu_msg)\n        self.balance_pub.publish(balance_vector)\n\n    def analyze_noise_characteristics(self):\n        """\n        Analyze noise characteristics of IMU data\n        """\n        if len(self.gyro_buffer) < 100:\n            return\n\n        gyro_data = np.array(list(self.gyro_buffer))\n        accel_data = np.array(list(self.accel_buffer))\n\n        # Calculate statistics\n        gyro_mean = np.mean(gyro_data, axis=0)\n        gyro_std = np.std(gyro_data, axis=0)\n        accel_mean = np.mean(accel_data, axis=0)\n        accel_std = np.std(accel_data, axis=0)\n\n        rospy.loginfo(f"IMU Noise Analysis:")\n        rospy.loginfo(f"  Gyro - Mean: {gyro_mean}, Std: {gyro_std}")\n        rospy.loginfo(f"  Accel - Mean: {accel_mean}, Std: {accel_std}")\n\n        return {\n            \'gyro_mean\': gyro_mean,\n            \'gyro_std\': gyro_std,\n            \'accel_mean\': accel_mean,\n            \'accel_std\': accel_std\n        }\n\nif __name__ == \'__main__\':\n    processor = AdvancedIMUProcessor()\n    rate = rospy.Rate(10)  # 10Hz analysis\n\n    while not rospy.is_shutdown():\n        # Periodically analyze noise characteristics\n        processor.analyze_noise_characteristics()\n        rate.sleep()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"imu-calibration-techniques",children:"IMU Calibration Techniques"}),"\n",(0,s.jsx)(n.p,{children:"For accurate humanoid robot operation, IMU calibration is essential:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Static Calibration"}),": Measure biases with the robot in known orientations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic Calibration"}),": Characterize scale factors and cross-axis sensitivities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temperature Calibration"}),": Account for temperature-dependent drift"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-position Calibration"}),": Calibrate across multiple orientations"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def perform_imu_calibration():\n    """\n    Example IMU calibration procedure\n    """\n    # Collect data in multiple static positions\n    positions = [\n        ("Level", [0, 0, 9.81]),      # Robot level, z-axis up\n        ("Roll 90", [0, 9.81, 0]),    # Robot rolled 90 degrees\n        ("Pitch 90", [9.81, 0, 0]),   # Robot pitched 90 degrees\n        ("Upside down", [0, 0, -9.81]) # Robot upside down\n    ]\n\n    calibration_data = {}\n    for name, expected_gravity in positions:\n        print(f"Position {name}: Please place robot as indicated and press Enter")\n        input()  # Wait for user to position robot\n\n        # Collect data for calibration\n        samples = []\n        for _ in range(1000):  # Collect 1000 samples\n            # In practice, read from IMU topic\n            # samples.append(read_imu_acceleration())\n            pass\n\n        calibration_data[name] = {\n            \'samples\': np.array(samples),\n            \'expected\': np.array(expected_gravity)\n        }\n\n    # Calculate calibration parameters\n    # This is a simplified example - real calibration is more complex\n    return calibration_data\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-techniques",children:"Sensor Fusion Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"kalman-filter-for-sensor-fusion",children:"Kalman Filter for Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Combining multiple sensors for improved state estimation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass SensorFusionKF:\n    def __init__(self):\n        # State: [x, y, z, vx, vy, vz, ax, ay, az]\n        self.state_dim = 9\n        self.obs_dim = 6  # Position + acceleration\n\n        # Initialize state and covariance\n        self.x = np.zeros(self.state_dim)\n        self.P = np.eye(self.state_dim) * 1000  # High initial uncertainty\n\n        # Process noise\n        self.Q = np.eye(self.state_dim) * 0.1\n\n        # Measurement noise\n        self.R = np.eye(self.obs_dim) * 0.5\n\n        # Observation matrix\n        self.H = np.zeros((self.obs_dim, self.state_dim))\n        # Position observations\n        self.H[0:3, 0:3] = np.eye(3)  # Position\n        self.H[3:6, 6:9] = np.eye(3)  # Acceleration\n\n    def predict(self, dt):\n        """\n        Prediction step of Kalman filter\n        """\n        # State transition matrix (simplified model)\n        F = np.eye(self.state_dim)\n        F[0:3, 3:6] = dt * np.eye(3)  # Position from velocity\n        F[3:6, 6:9] = dt * np.eye(3)  # Velocity from acceleration\n\n        # Predict state\n        self.x = F @ self.x\n\n        # Predict covariance\n        self.P = F @ self.P @ F.T + self.Q\n\n    def update(self, measurement):\n        """\n        Update step of Kalman filter\n        """\n        # Innovation\n        y = measurement - self.H @ self.x\n\n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n\n        # Update state\n        self.x = self.x + K @ y\n\n        # Update covariance\n        I = np.eye(self.state_dim)\n        self.P = (I - K @ self.H) @ self.P\n\ndef sensor_fusion_example():\n    """\n    Example of fusing IMU and camera data\n    """\n    kf = SensorFusionKF()\n\n    # Simulated measurements\n    position = np.array([1.0, 2.0, 0.5])\n    acceleration = np.array([0.1, -0.05, 0.02])\n    measurement = np.concatenate([position, acceleration])\n\n    # Time step\n    dt = 0.01\n\n    # Prediction and update\n    kf.predict(dt)\n    kf.update(measurement)\n\n    return kf.x  # Fused state estimate\n'})}),"\n",(0,s.jsx)(n.h2,{id:"validation-methods",children:"Validation Methods"}),"\n",(0,s.jsx)(n.h3,{id:"comparing-simulated-vs-real-data",children:"Comparing Simulated vs. Real Data"}),"\n",(0,s.jsx)(n.p,{children:"Validating sensor simulation accuracy is crucial for trust in the simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef validate_sensor_simulation(simulated_data, real_data, sensor_type):\n    \"\"\"\n    Validate simulated sensor data against real data\n    \"\"\"\n    # Calculate statistical measures\n    sim_mean = np.mean(simulated_data, axis=0)\n    real_mean = np.mean(real_data, axis=0)\n\n    sim_std = np.std(simulated_data, axis=0)\n    real_std = np.std(real_data, axis=0)\n\n    # Calculate correlation\n    correlation = np.corrcoef(simulated_data.flatten(), real_data.flatten())[0, 1]\n\n    # Calculate RMSE\n    rmse = np.sqrt(np.mean((simulated_data - real_data) ** 2))\n\n    print(f\"Sensor Validation Results ({sensor_type}):\")\n    print(f\"  Mean Difference: {np.abs(sim_mean - real_mean)}\")\n    print(f\"  Std Dev Difference: {np.abs(sim_std - real_std)}\")\n    print(f\"  Correlation: {correlation:.4f}\")\n    print(f\"  RMSE: {rmse:.4f}\")\n\n    # Plot comparison\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 2, 1)\n    plt.plot(simulated_data[:, 0], label='Simulated', alpha=0.7)\n    plt.plot(real_data[:, 0], label='Real', alpha=0.7)\n    plt.title(f'{sensor_type} - Axis 1 Comparison')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(simulated_data[:, 1], label='Simulated', alpha=0.7)\n    plt.plot(real_data[:, 1], label='Real', alpha=0.7)\n    plt.title(f'{sensor_type} - Axis 2 Comparison')\n    plt.legend()\n\n    plt.subplot(2, 2, 3)\n    plt.scatter(real_data.flatten(), simulated_data.flatten(), alpha=0.5)\n    plt.plot([real_data.min(), real_data.max()], [real_data.min(), real_data.max()], 'r--')\n    plt.xlabel('Real Data')\n    plt.ylabel('Simulated Data')\n    plt.title('Scatter Plot (Perfect = Diagonal)')\n\n    plt.subplot(2, 2, 4)\n    plt.hist(simulated_data.flatten() - real_data.flatten(), bins=50, alpha=0.7, label='Simulated - Real')\n    plt.title('Error Distribution')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'correlation': correlation,\n        'rmse': rmse,\n        'mean_error': np.abs(sim_mean - real_mean),\n        'std_error': np.abs(sim_std - real_std)\n    }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR Simulation"}),": Configure a LiDAR sensor in Gazebo and analyze the point cloud data it generates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU Processing"}),": Implement an IMU bias estimation algorithm and test with simulated data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion"}),": Combine data from multiple simulated sensors using a Kalman filter"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises-1",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-lidar-sensor-configuration",children:"Exercise 1: LiDAR Sensor Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Configure and test a LiDAR sensor for humanoid robot navigation:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up a 2D LiDAR sensor with appropriate range and resolution for indoor navigation"}),"\n",(0,s.jsx)(n.li,{children:"Configure realistic noise models including Gaussian noise and bias"}),"\n",(0,s.jsx)(n.li,{children:"Implement a simple obstacle detection algorithm using the simulated LiDAR data"}),"\n",(0,s.jsx)(n.li,{children:"Test the sensor in various environments with different obstacle types"}),"\n",(0,s.jsx)(n.li,{children:"Analyze the impact of different noise parameters on detection performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-depth-camera-integration",children:"Exercise 2: Depth Camera Integration"}),"\n",(0,s.jsx)(n.p,{children:"Integrate a depth camera into the humanoid robot simulation:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure RGB-D camera with appropriate intrinsics and distortion parameters"}),"\n",(0,s.jsx)(n.li,{children:"Implement object detection using both color and depth information"}),"\n",(0,s.jsx)(n.li,{children:"Create a point cloud processing pipeline for environment understanding"}),"\n",(0,s.jsx)(n.li,{children:"Test the system with various objects at different distances"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the accuracy of depth measurements compared to ground truth"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-imu-calibration-and-fusion",children:"Exercise 3: IMU Calibration and Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Calibrate and fuse IMU data for humanoid balance:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure multiple IMUs at different locations on the robot (head, torso, feet)"}),"\n",(0,s.jsx)(n.li,{children:"Implement bias estimation and correction algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Develop a sensor fusion algorithm combining gyroscope and accelerometer data"}),"\n",(0,s.jsx)(n.li,{children:"Test the system during various robot motions (standing, walking, turning)"}),"\n",(0,s.jsx)(n.li,{children:"Validate the orientation estimates against ground truth data"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-4-multi-sensor-integration",children:"Exercise 4: Multi-Sensor Integration"}),"\n",(0,s.jsx)(n.p,{children:"Integrate multiple sensors for comprehensive environment perception:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Combine LiDAR, depth camera, and IMU data in a single perception pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion techniques to improve perception accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Create a unified representation of the environment using all sensor inputs"}),"\n",(0,s.jsx)(n.li,{children:"Test the integrated system in complex scenarios with dynamic obstacles"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the robustness of the multi-sensor system compared to single sensors"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"validation-and-troubleshooting-tips",children:"Validation and Troubleshooting Tips"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-simulation-validation-techniques",children:"Sensor Simulation Validation Techniques"}),"\n",(0,s.jsx)(n.p,{children:"Proper validation ensures that simulated sensors behave similarly to their real-world counterparts:"}),"\n",(0,s.jsx)(n.h4,{id:"cross-validation-with-real-sensors",children:"Cross-Validation with Real Sensors"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Comparison"}),": Compare statistical properties of simulated vs. real sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Metrics"}),": Evaluate algorithms on both simulated and real data to ensure similar performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transfer Learning"}),": Test if models trained on simulated data perform well on real data"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"common-validation-metrics",children:"Common Validation Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR"}),": Point cloud density, range accuracy, angular resolution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Cameras"}),": Depth accuracy, FoV coverage, noise characteristics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMUs"}),": Bias stability, noise spectral density, scale factor accuracy"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"simulation-to-reality-gap-assessment",children:"Simulation-to-Reality Gap Assessment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def assess_sim2real_gap(simulated_data, real_data, sensor_type):\n    \"\"\"\n    Assess the gap between simulated and real sensor data\n    \"\"\"\n    import numpy as np\n    from scipy import stats\n\n    # Calculate statistical measures\n    sim_stats = {\n        'mean': np.mean(simulated_data, axis=0),\n        'std': np.std(simulated_data, axis=0),\n        'min': np.min(simulated_data, axis=0),\n        'max': np.max(simulated_data, axis=0)\n    }\n\n    real_stats = {\n        'mean': np.mean(real_data, axis=0),\n        'std': np.std(real_data, axis=0),\n        'min': np.min(real_data, axis=0),\n        'max': np.max(real_data, axis=0)\n    }\n\n    # Calculate similarity metrics\n    mean_diff = np.abs(sim_stats['mean'] - real_stats['mean'])\n    std_diff = np.abs(sim_stats['std'] - real_stats['std'])\n\n    # Kolmogorov-Smirnov test for distribution similarity\n    ks_statistic, p_value = stats.ks_2samp(\n        simulated_data.flatten(),\n        real_data.flatten()\n    )\n\n    print(f\"Sensor Type: {sensor_type}\")\n    print(f\"Mean Difference: {mean_diff}\")\n    print(f\"Std Dev Difference: {std_diff}\")\n    print(f\"KS Test Statistic: {ks_statistic:.4f}\")\n    print(f\"KS Test P-Value: {p_value:.4f}\")\n    print(f\"Distributions Similar: {p_value > 0.05}\")\n\n    return {\n        'mean_diff': mean_diff,\n        'std_diff': std_diff,\n        'ks_statistic': ks_statistic,\n        'p_value': p_value\n    }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,s.jsx)(n.h4,{id:"lidar-simulation-issues",children:"LiDAR Simulation Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point Cloud Artifacts"}),": Check for incorrect noise parameters or range settings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Problems"}),": Reduce sample count or update rate if simulation is slow"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Inconsistent Data"}),": Verify sensor mounting position and orientation"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"depth-camera-issues",children:"Depth Camera Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Invalid Depth Values"}),": Check for proper clipping distances and noise modeling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Registration Problems"}),": Ensure RGB and depth images are properly synchronized"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Artifacts"}),": Adjust noise parameters to match real sensor characteristics"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"imu-simulation-issues",children:"IMU Simulation Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Drift Accumulation"}),": Verify bias estimation and correction algorithms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration Errors"}),": Check time synchronization and update rates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gravity Compensation"}),": Ensure proper accelerometer calibration"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"general-troubleshooting-tips",children:"General Troubleshooting Tips"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Start Simple"}),": Begin with basic configurations before adding complexity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verify Parameters"}),": Double-check all sensor parameters against real hardware specs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Performance"}),": Keep track of simulation update rates and computational load"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validate Step-by-Step"}),": Test each sensor component individually before integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Document Assumptions"}),": Keep track of modeling assumptions and limitations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h4,{id:"sensor-specific-optimizations",children:"Sensor-Specific Optimizations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR"}),": Reduce scan resolution when precision isn't critical"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"}),": Use lower resolution or frame rate when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMUs"}),": Use appropriate update rates (balance accuracy vs. performance)"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"integration-optimizations",children:"Integration Optimizations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Filtering"}),": Pre-filter sensor data to reduce computational load"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel Processing"}),": Process multiple sensors in parallel when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Data Structures"}),": Use appropriate data structures for sensor data"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered the simulation of various sensors used in humanoid robotics, including LiDAR, depth cameras, and IMUs. We explored configuration, noise modeling, data processing, and validation techniques. Proper sensor simulation is essential for developing robust perception algorithms that can transfer from simulation to reality."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>o});var a=i(6540);const s={},t=a.createContext(s);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);