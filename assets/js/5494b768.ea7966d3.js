"use strict";(globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend=globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend||[]).push([[4],{3358(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"vla-llm-integration/voice-to-action-whisper","title":"Voice-to-Action with OpenAI Whisper","description":"Introduction","source":"@site/docs/vla-llm-integration/voice-to-action-whisper.md","sourceDirName":"vla-llm-integration","slug":"/vla-llm-integration/voice-to-action-whisper","permalink":"/docs/vla-llm-integration/voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-llm-integration/voice-to-action-whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 Path Planning for Humanoid Robots","permalink":"/docs/isaac-ai-brain/nav2-path-planning-humanoid"},"next":{"title":"Cognitive Planning using LLMs for ROS 2","permalink":"/docs/vla-llm-integration/cognitive-planning-llms-ros2"}}');var a=i(4848),o=i(8453);const t={},r="Voice-to-Action with OpenAI Whisper",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding OpenAI Whisper",id:"understanding-openai-whisper",level:2},{value:"Overview of Whisper Technology",id:"overview-of-whisper-technology",level:3},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"Key Features",id:"key-features",level:3},{value:"Installing and Setting Up Whisper",id:"installing-and-setting-up-whisper",level:2},{value:"Installation Options",id:"installation-options",level:3},{value:"Option 1: OpenAI API (Recommended for beginners)",id:"option-1-openai-api-recommended-for-beginners",level:4},{value:"Option 2: OpenAI&#39;s Whisper Library (For local processing)",id:"option-2-openais-whisper-library-for-local-processing",level:4},{value:"Option 3: Hugging Face Transformers (Alternative implementation)",id:"option-3-hugging-face-transformers-alternative-implementation",level:4},{value:"Basic Setup and Configuration",id:"basic-setup-and-configuration",level:3},{value:"Local Whisper Installation (For advanced users)",id:"local-whisper-installation-for-advanced-users",level:3},{value:"Audio Input and Preprocessing",id:"audio-input-and-preprocessing",level:2},{value:"Audio Capture Setup",id:"audio-capture-setup",level:3},{value:"Audio Preprocessing for Whisper",id:"audio-preprocessing-for-whisper",level:3},{value:"Real-time Voice Processing Pipeline",id:"real-time-voice-processing-pipeline",level:2},{value:"Voice Activity Detection",id:"voice-activity-detection",level:3},{value:"Complete Voice Processing Pipeline",id:"complete-voice-processing-pipeline",level:3},{value:"Integration with Robotic Systems",id:"integration-with-robotic-systems",level:2},{value:"Voice Command Validation",id:"voice-command-validation",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Performance Optimization and Tuning",id:"performance-optimization-and-tuning",level:2},{value:"Whisper Model Selection",id:"whisper-model-selection",level:3},{value:"Audio Quality Optimization",id:"audio-quality-optimization",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Basic Voice Command System",id:"example-1-basic-voice-command-system",level:3},{value:"Example 2: Voice Command with Context Awareness",id:"example-2-voice-command-with-context-awareness",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"This chapter introduces OpenAI Whisper as the foundation for voice-to-action systems in humanoid robotics. Whisper is a state-of-the-art speech recognition model that can convert spoken language into text with high accuracy. In the context of humanoid robots, Whisper enables natural human-robot interaction by allowing users to communicate with robots using everyday speech."}),"\n",(0,a.jsx)(n.p,{children:"The chapter will cover the installation and configuration of Whisper, integration with audio input systems, and processing of voice commands for robotic action execution. You'll learn how to set up real-time voice processing pipelines and validate the accuracy of voice-to-text conversion in various environments."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Install and configure OpenAI Whisper for voice processing applications"}),"\n",(0,a.jsx)(n.li,{children:"Set up audio input systems for real-time voice command processing"}),"\n",(0,a.jsx)(n.li,{children:"Process voice commands with Whisper and validate output accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Handle different audio conditions and optimize Whisper performance"}),"\n",(0,a.jsx)(n.li,{children:"Integrate Whisper with robotic control systems for action execution"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Basic understanding of audio processing concepts"}),"\n",(0,a.jsx)(n.li,{children:"Familiarity with Python programming"}),"\n",(0,a.jsx)(n.li,{children:"Access to OpenAI API keys for Whisper usage"}),"\n",(0,a.jsx)(n.li,{children:"Basic knowledge of ROS 2 for later integration"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"understanding-openai-whisper",children:"Understanding OpenAI Whisper"}),"\n",(0,a.jsx)(n.h3,{id:"overview-of-whisper-technology",children:"Overview of Whisper Technology"}),"\n",(0,a.jsx)(n.p,{children:"OpenAI Whisper is a robust speech-to-text model that demonstrates strong performance across multiple languages and domains. The model is designed to handle various audio conditions including:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Different accents and speaking styles"}),"\n",(0,a.jsx)(n.li,{children:"Background noise and environmental conditions"}),"\n",(0,a.jsx)(n.li,{children:"Various recording qualities and equipment"}),"\n",(0,a.jsx)(n.li,{children:"Multi-language support (98+ languages)"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,a.jsx)(n.p,{children:"Whisper comes in several model sizes with different performance characteristics:"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Model"}),(0,a.jsx)(n.th,{children:"Size"}),(0,a.jsx)(n.th,{children:"Required VRAM"}),(0,a.jsx)(n.th,{children:"Relative Speed"}),(0,a.jsx)(n.th,{children:"English-only"}),(0,a.jsx)(n.th,{children:"Multilingual"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"tiny"}),(0,a.jsx)(n.td,{children:"75 MB"}),(0,a.jsx)(n.td,{children:"~1 GB"}),(0,a.jsx)(n.td,{children:"32x"}),(0,a.jsx)(n.td,{children:"3.0%"}),(0,a.jsx)(n.td,{children:"4.0%"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"base"}),(0,a.jsx)(n.td,{children:"145 MB"}),(0,a.jsx)(n.td,{children:"~1 GB"}),(0,a.jsx)(n.td,{children:"16x"}),(0,a.jsx)(n.td,{children:"2.9%"}),(0,a.jsx)(n.td,{children:"3.5%"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"small"}),(0,a.jsx)(n.td,{children:"445 MB"}),(0,a.jsx)(n.td,{children:"~2 GB"}),(0,a.jsx)(n.td,{children:"6x"}),(0,a.jsx)(n.td,{children:"2.1%"}),(0,a.jsx)(n.td,{children:"2.6%"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"medium"}),(0,a.jsx)(n.td,{children:"830 MB"}),(0,a.jsx)(n.td,{children:"~5 GB"}),(0,a.jsx)(n.td,{children:"2x"}),(0,a.jsx)(n.td,{children:"1.9%"}),(0,a.jsx)(n.td,{children:"2.7%"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"large"}),(0,a.jsx)(n.td,{children:"1.5 GB"}),(0,a.jsx)(n.td,{children:"~10 GB"}),(0,a.jsx)(n.td,{children:"1x"}),(0,a.jsx)(n.td,{children:"1.8%"}),(0,a.jsx)(n.td,{children:"2.9%"})]})]})]}),"\n",(0,a.jsx)(n.p,{children:"*Based on LibriSpeech dev-clean dataset. Speed measured on Intel i9-12900K CPU."}),"\n",(0,a.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multilingual Support"}),": Can transcribe and translate between languages"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robustness"}),": Handles various audio conditions and accents"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Timestamps"}),": Provides precise timing information for speech segments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Punctuation"}),": Automatically adds punctuation to transcriptions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Speaker Diarization"}),": Can distinguish between different speakers"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"installing-and-setting-up-whisper",children:"Installing and Setting Up Whisper"}),"\n",(0,a.jsx)(n.h3,{id:"installation-options",children:"Installation Options"}),"\n",(0,a.jsx)(n.p,{children:"Whisper can be used in multiple ways depending on your requirements:"}),"\n",(0,a.jsx)(n.h4,{id:"option-1-openai-api-recommended-for-beginners",children:"Option 1: OpenAI API (Recommended for beginners)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,a.jsx)(n.h4,{id:"option-2-openais-whisper-library-for-local-processing",children:"Option 2: OpenAI's Whisper Library (For local processing)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,a.jsx)(n.h4,{id:"option-3-hugging-face-transformers-alternative-implementation",children:"Option 3: Hugging Face Transformers (Alternative implementation)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install transformers torch\n"})}),"\n",(0,a.jsx)(n.h3,{id:"basic-setup-and-configuration",children:"Basic Setup and Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Basic Whisper setup with OpenAI API\nimport openai\nimport os\n\n# Set your OpenAI API key\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\ndef transcribe_audio_file(audio_file_path):\n    """\n    Transcribe an audio file using OpenAI Whisper API\n    """\n    with open(audio_file_path, "rb") as audio_file:\n        transcript = openai.Audio.transcribe(\n            model="whisper-1",\n            file=audio_file,\n            response_format="text"\n        )\n    return transcript\n\n# Example usage\ntranscription = transcribe_audio_file("command.wav")\nprint(f"Transcription: {transcription}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"local-whisper-installation-for-advanced-users",children:"Local Whisper Installation (For advanced users)"}),"\n",(0,a.jsx)(n.p,{children:"For local processing without API calls:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Local Whisper setup\nimport whisper\n\n# Load different model sizes\nmodel_tiny = whisper.load_model("tiny")\nmodel_base = whisper.load_model("base")\nmodel_small = whisper.load_model("small")\nmodel_medium = whisper.load_model("medium")\nmodel_large = whisper.load_model("large")\n\ndef transcribe_with_local_model(audio_path, model_size="small"):\n    """\n    Transcribe audio using local Whisper model\n    """\n    model = whisper.load_model(model_size)\n\n    # Load audio and pad/trim it to fit 30-second context\n    audio = whisper.load_audio(audio_path)\n    audio = whisper.pad_or_trim(audio)\n\n    # Make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    # Detect the spoken language with the largest probability\n    _, probs = model.detect_language(mel)\n    detected_lang = max(probs, key=probs.get)\n\n    # Decode the audio\n    options = whisper.DecodingOptions()\n    result = whisper.decode(model, mel, options)\n\n    return {\n        "text": result.text,\n        "language": detected_lang,\n        "segments": result.segments if hasattr(result, \'segments\') else []\n    }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"audio-input-and-preprocessing",children:"Audio Input and Preprocessing"}),"\n",(0,a.jsx)(n.h3,{id:"audio-capture-setup",children:"Audio Capture Setup"}),"\n",(0,a.jsx)(n.p,{children:"For real-time voice command processing, you'll need to set up audio capture:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Real-time audio capture for Whisper\nimport pyaudio\nimport wave\nimport numpy as np\nimport threading\nimport time\nfrom datetime import datetime\n\nclass AudioRecorder:\n    def __init__(self, chunk=1024, format=pyaudio.paInt16, channels=1, rate=16000):\n        self.chunk = chunk\n        self.format = format\n        self.channels = channels\n        self.rate = rate\n        self.p = pyaudio.PyAudio()\n\n        # Initialize recording parameters\n        self.frames = []\n        self.is_recording = False\n        self.recording_thread = None\n\n    def start_recording(self, filename=None):\n        """\n        Start recording audio from microphone\n        """\n        if filename is None:\n            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n            filename = f"recording_{timestamp}.wav"\n\n        self.is_recording = True\n        self.frames = []\n\n        # Open stream\n        self.stream = self.p.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        # Start recording in a separate thread\n        self.recording_thread = threading.Thread(target=self._record_audio)\n        self.recording_thread.start()\n\n        return filename\n\n    def stop_recording(self, filename):\n        """\n        Stop recording and save to file\n        """\n        self.is_recording = False\n\n        if self.recording_thread:\n            self.recording_thread.join()\n\n        # Save the recorded data as a WAV file\n        wf = wave.open(filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(self.p.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(self.frames))\n        wf.close()\n\n        self.stream.stop_stream()\n        self.stream.close()\n\n        return filename\n\n    def _record_audio(self):\n        """\n        Internal method to record audio in a loop\n        """\n        while self.is_recording:\n            data = self.stream.read(self.chunk)\n            self.frames.append(data)\n\n# Example usage\nrecorder = AudioRecorder()\nfilename = recorder.start_recording()\ntime.sleep(5)  # Record for 5 seconds\nrecorder.stop_recording(filename)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"audio-preprocessing-for-whisper",children:"Audio Preprocessing for Whisper"}),"\n",(0,a.jsx)(n.p,{children:"Whisper works best with specific audio formats. Here's how to preprocess audio:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: Audio preprocessing for optimal Whisper performance\nimport librosa\nimport soundfile as sf\nimport numpy as np\n\ndef preprocess_audio_for_whisper(input_file, output_file=None):\n    \"\"\"\n    Preprocess audio file to optimize for Whisper transcription\n    \"\"\"\n    if output_file is None:\n        output_file = input_file.replace('.wav', '_processed.wav')\n\n    # Load audio file\n    audio, sr = librosa.load(input_file, sr=16000, mono=True)\n\n    # Normalize audio\n    audio = audio / np.max(np.abs(audio))\n\n    # Apply slight noise reduction (optional)\n    # This can help with background noise\n    from scipy import signal\n    b, a = signal.butter(8, 0.04, 'highpass')\n    audio = signal.filtfilt(b, a, audio)\n\n    # Save processed audio\n    sf.write(output_file, audio, 16000)\n\n    return output_file\n\ndef validate_audio_quality(audio_file):\n    \"\"\"\n    Validate audio quality for Whisper processing\n    \"\"\"\n    audio, sr = librosa.load(audio_file)\n\n    # Check if audio meets minimum requirements\n    duration = len(audio) / sr\n    rms = np.sqrt(np.mean(audio**2))\n\n    quality_metrics = {\n        'duration': duration,\n        'sample_rate': sr,\n        'rms_amplitude': rms,\n        'is_silent': rms < 0.001,\n        'is_too_long': duration > 25  # Whisper is optimized for shorter clips\n    }\n\n    return quality_metrics\n"})}),"\n",(0,a.jsx)(n.h2,{id:"real-time-voice-processing-pipeline",children:"Real-time Voice Processing Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"voice-activity-detection",children:"Voice Activity Detection"}),"\n",(0,a.jsx)(n.p,{children:"Implementing voice activity detection to trigger processing only when speech is detected:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Voice Activity Detection (VAD) for Whisper\nimport webrtcvad\nimport collections\n\nclass VoiceActivityDetector:\n    def __init__(self, aggressiveness=3, sample_rate=16000, frame_duration_ms=30):\n        self.vad = webrtcvad.Vad(aggressiveness)\n        self.sample_rate = sample_rate\n        self.frame_duration_ms = frame_duration_ms\n        self.frame_size = int(sample_rate * frame_duration_ms / 1000) * 2  # 2 bytes per sample\n\n        # For voice activity detection\n        self.ring_buffer = collections.deque(maxlen=30)\n        self.triggered = False\n        self.vad_frames = []\n        self.temp_end = 0\n        self.max_silence_for_end = 30  # frames\n\n    def is_speech(self, audio_frame):\n        """\n        Check if the audio frame contains speech\n        """\n        try:\n            return self.vad.is_speech(audio_frame, self.sample_rate)\n        except:\n            return False\n\n    def process_audio_chunk(self, audio_chunk):\n        """\n        Process audio chunk for voice activity detection\n        """\n        speech_detected = self.is_speech(audio_chunk)\n\n        if not self.triggered:\n            self.ring_buffer.append((audio_chunk, speech_detected))\n            num_voiced = len([f for f, speech in self.ring_buffer if speech])\n\n            # If we\'re getting more voice than silence\n            if num_voiced > 0.8 * self.ring_buffer.maxlen:\n                self.triggered = True\n                self.vad_frames = [f for f, s in self.ring_buffer if s]\n                self.ring_buffer.clear()\n        else:\n            self.vad_frames.append(audio_chunk)\n            self.ring_buffer.append((audio_chunk, speech_detected))\n            num_unvoiced = len([f for f, speech in self.ring_buffer if not speech])\n\n            # If more than 90% of the frames in the ring buffer are unvoiced\n            if num_unvoiced > 0.9 * self.ring_buffer.maxlen:\n                self.temp_end = len(self.vad_frames)\n            if self.temp_end != 0 and num_unvoiced > self.max_silence_for_end:\n                self.triggered = False\n                temp_vad_frames = self.vad_frames\n                self.vad_frames = []\n                self.temp_end = 0\n                return True, temp_vad_frames  # Return the speech segment\n\n        return False, []\n'})}),"\n",(0,a.jsx)(n.h3,{id:"complete-voice-processing-pipeline",children:"Complete Voice Processing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Complete voice-to-text processing pipeline\nimport asyncio\nimport tempfile\nimport os\nfrom pydub import AudioSegment\n\nclass WhisperVoiceProcessor:\n    def __init__(self, model_size="small", use_api=True):\n        self.use_api = use_api\n        self.model_size = model_size\n\n        if not use_api:\n            import whisper\n            self.model = whisper.load_model(model_size)\n\n        self.audio_recorder = AudioRecorder()\n        self.vad_detector = VoiceActivityDetector()\n\n    async def process_voice_command(self, timeout=10):\n        """\n        Process a voice command from start to finish\n        """\n        # Start recording\n        temp_file = tempfile.mktemp(suffix=\'.wav\')\n        recording_file = self.audio_recorder.start_recording(temp_file)\n\n        # Wait for voice activity or timeout\n        start_time = time.time()\n        speech_segments = []\n\n        while time.time() - start_time < timeout:\n            # In a real implementation, you\'d continuously check for speech\n            # This is simplified for the example\n            pass\n\n        # Stop recording\n        self.audio_recorder.stop_recording(recording_file)\n\n        # Process the recorded audio\n        processed_file = preprocess_audio_for_whisper(recording_file)\n\n        # Transcribe with Whisper\n        if self.use_api:\n            transcription = await self.transcribe_with_api(processed_file)\n        else:\n            transcription = await self.transcribe_locally(processed_file)\n\n        # Clean up temporary files\n        os.remove(recording_file)\n        os.remove(processed_file)\n\n        return transcription\n\n    async def transcribe_with_api(self, audio_file):\n        """\n        Transcribe audio using OpenAI API\n        """\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            transcribe_audio_file,\n            audio_file\n        )\n\n    async def transcribe_locally(self, audio_file):\n        """\n        Transcribe audio using local Whisper model\n        """\n        result = transcribe_with_local_model(audio_file, self.model_size)\n        return result["text"]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-robotic-systems",children:"Integration with Robotic Systems"}),"\n",(0,a.jsx)(n.h3,{id:"voice-command-validation",children:"Voice Command Validation"}),"\n",(0,a.jsx)(n.p,{children:"Validating voice commands before passing them to the robotic system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: Voice command validation and filtering\nimport re\nfrom typing import List, Dict\n\nclass VoiceCommandValidator:\n    def __init__(self):\n        # Define safe commands that the robot can execute\n        self.allowed_commands = {\n            'navigation': [\n                'go forward', 'move forward', 'go back', 'move back',\n                'turn left', 'turn right', 'go to', 'move to', 'stop',\n                'halt', 'pause', 'continue'\n            ],\n            'manipulation': [\n                'pick up', 'grasp', 'grab', 'release', 'drop',\n                'lift', 'lower', 'open', 'close'\n            ],\n            'communication': [\n                'speak', 'say', 'hello', 'goodbye', 'thank you'\n            ]\n        }\n\n        # Define potentially dangerous commands to filter\n        self.dangerous_keywords = [\n            'self-destruct', 'shutdown', 'power off', 'emergency stop',\n            'kill', 'destroy', 'harm', 'danger'\n        ]\n\n    def validate_command(self, transcription: str) -> Dict:\n        \"\"\"\n        Validate a voice command for safety and executability\n        \"\"\"\n        command = transcription.lower().strip()\n        validation_result = {\n            'is_valid': False,\n            'is_safe': True,\n            'command_type': None,\n            'parsed_command': None,\n            'confidence': 0.0\n        }\n\n        # Check for dangerous keywords\n        for keyword in self.dangerous_keywords:\n            if keyword in command:\n                validation_result['is_safe'] = False\n                return validation_result\n\n        # Try to match against allowed commands\n        for cmd_type, commands in self.allowed_commands.items():\n            for allowed_cmd in commands:\n                if allowed_cmd in command:\n                    validation_result['is_valid'] = True\n                    validation_result['command_type'] = cmd_type\n                    validation_result['parsed_command'] = allowed_cmd\n                    validation_result['confidence'] = 0.8  # Base confidence\n\n                    # Calculate more specific confidence based on match quality\n                    if command == allowed_cmd:\n                        validation_result['confidence'] = 1.0\n                    elif allowed_cmd in command and len(command.split()) <= len(allowed_cmd.split()) + 2:\n                        validation_result['confidence'] = 0.9\n\n                    break\n            if validation_result['is_valid']:\n                break\n\n        return validation_result\n\n    def extract_parameters(self, command: str, command_type: str) -> Dict:\n        \"\"\"\n        Extract parameters from voice command\n        \"\"\"\n        params = {}\n\n        if command_type == 'navigation':\n            # Extract location or distance from navigation commands\n            distance_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(meters?|m|feet|ft)', command)\n            if distance_match:\n                params['distance'] = float(distance_match.group(1))\n                params['unit'] = distance_match.group(2)\n\n            # Extract destination from \"go to\" commands\n            to_match = re.search(r'go to\\s+(.+)|move to\\s+(.+)', command)\n            if to_match:\n                destination = to_match.group(1) or to_match.group(2)\n                params['destination'] = destination.strip()\n\n        elif command_type == 'manipulation':\n            # Extract object from manipulation commands\n            object_match = re.search(r'(?:pick up|grasp|grab|lift)\\s+(.+)', command)\n            if object_match:\n                params['object'] = object_match.group(1).strip()\n\n        return params\n"})}),"\n",(0,a.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,a.jsx)(n.p,{children:"Integrating voice processing with ROS 2 for robotic action execution:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: ROS 2 integration for voice-controlled robot actions\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import AudioData\nimport json\n\nclass VoiceControlNode(Node):\n    def __init__(self):\n        super().__init__('voice_control_node')\n\n        # Publishers for different robot actions\n        self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.speech_publisher = self.create_publisher(String, '/robot_speech', 10)\n\n        # Subscriber for voice commands\n        self.voice_subscriber = self.create_subscription(\n            String, '/voice_commands', self.voice_command_callback, 10\n        )\n\n        # Initialize command validator\n        self.validator = VoiceCommandValidator()\n\n        self.get_logger().info('Voice control node initialized')\n\n    def voice_command_callback(self, msg):\n        \"\"\"\n        Process incoming voice command\n        \"\"\"\n        command_text = msg.data\n\n        # Validate the command\n        validation = self.validator.validate_command(command_text)\n\n        if not validation['is_valid']:\n            self.get_logger().warn(f'Invalid command: {command_text}')\n            return\n\n        if not validation['is_safe']:\n            self.get_logger().error(f'Dangerous command detected: {command_text}')\n            return\n\n        # Extract parameters\n        params = self.validator.extract_parameters(command_text, validation['command_type'])\n\n        # Execute appropriate action based on command type\n        if validation['command_type'] == 'navigation':\n            self.execute_navigation_command(command_text, params)\n        elif validation['command_type'] == 'manipulation':\n            self.execute_manipulation_command(command_text, params)\n        elif validation['command_type'] == 'communication':\n            self.execute_communication_command(command_text, params)\n\n    def execute_navigation_command(self, command, params):\n        \"\"\"\n        Execute navigation-related voice commands\n        \"\"\"\n        twist = Twist()\n\n        if 'forward' in command or 'move' in command:\n            twist.linear.x = 0.5  # Move forward at 0.5 m/s\n        elif 'back' in command:\n            twist.linear.x = -0.5  # Move backward\n        elif 'left' in command:\n            twist.angular.z = 0.5  # Turn left\n        elif 'right' in command:\n            twist.angular.z = -0.5  # Turn right\n        elif 'stop' in command or 'halt' in command:\n            twist.linear.x = 0.0\n            twist.angular.z = 0.0\n\n        self.cmd_vel_publisher.publish(twist)\n        self.get_logger().info(f'Executed navigation command: {command}')\n\n    def execute_communication_command(self, command, params):\n        \"\"\"\n        Execute communication-related voice commands\n        \"\"\"\n        response_msg = String()\n\n        if 'hello' in command:\n            response_msg.data = \"Hello! How can I assist you today?\"\n        elif 'thank you' in command:\n            response_msg.data = \"You're welcome! Happy to help.\"\n        elif 'goodbye' in command:\n            response_msg.data = \"Goodbye! Have a great day.\"\n        else:\n            response_msg.data = f\"I heard you say: {command}\"\n\n        self.speech_publisher.publish(response_msg)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization-and-tuning",children:"Performance Optimization and Tuning"}),"\n",(0,a.jsx)(n.h3,{id:"whisper-model-selection",children:"Whisper Model Selection"}),"\n",(0,a.jsx)(n.p,{children:"Choosing the right Whisper model based on your requirements:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: Model selection based on performance requirements\nclass WhisperModelSelector:\n    def __init__(self):\n        self.models = {\n            'tiny': {'size': 75, 'vram': 1, 'speed': 32, 'accuracy': 0.96},\n            'base': {'size': 145, 'vram': 1, 'speed': 16, 'accuracy': 0.97},\n            'small': {'size': 445, 'vram': 2, 'speed': 6, 'accuracy': 0.979},\n            'medium': {'size': 830, 'vram': 5, 'speed': 2, 'accuracy': 0.981},\n            'large': {'size': 1500, 'vram': 10, 'speed': 1, 'accuracy': 0.971}\n        }\n\n    def recommend_model(self, requirements):\n        \"\"\"\n        Recommend the best Whisper model based on requirements\n        \"\"\"\n        vram_available = requirements.get('vram_gb', 8)\n        speed_requirement = requirements.get('min_speed_x', 10)  # Minimum speed multiplier\n        accuracy_requirement = requirements.get('min_accuracy', 0.95)\n        max_size_mb = requirements.get('max_model_size_mb', 1000)\n\n        best_model = None\n        for model_name, specs in self.models.items():\n            if (specs['vram'] <= vram_available and\n                specs['speed'] >= speed_requirement and\n                specs['accuracy'] >= accuracy_requirement and\n                specs['size'] <= max_size_mb):\n\n                if best_model is None or self.models[best_model]['accuracy'] < specs['accuracy']:\n                    best_model = model_name\n\n        return best_model or 'base'  # Default fallback\n\n# Example usage\nselector = WhisperModelSelector()\nrequirements = {\n    'vram_gb': 6,\n    'min_speed_x': 5,\n    'min_accuracy': 0.97,\n    'max_model_size_mb': 500\n}\nrecommended_model = selector.recommend_model(requirements)\nprint(f\"Recommended model: {recommended_model}\")\n"})}),"\n",(0,a.jsx)(n.h3,{id:"audio-quality-optimization",children:"Audio Quality Optimization"}),"\n",(0,a.jsx)(n.p,{children:"Optimizing audio input for better Whisper performance:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example: Audio quality optimization techniques\nclass AudioQualityOptimizer:\n    def __init__(self):\n        self.noise_threshold = 0.01\n        self.silence_threshold = 0.001\n        self.min_speech_duration = 0.2  # seconds\n\n    def optimize_audio_input(self, audio_data, sample_rate=16000):\n        \"\"\"\n        Optimize audio input for Whisper processing\n        \"\"\"\n        import librosa\n        import numpy as np\n\n        # Normalize audio\n        audio_data = audio_data / np.max(np.abs(audio_data))\n\n        # Apply noise reduction if needed\n        if np.std(audio_data) > self.noise_threshold:\n            # Simple spectral gating for noise reduction\n            from scipy import signal\n            b, a = signal.butter(8, 0.04, 'highpass')\n            audio_data = signal.filtfilt(b, a, audio_data)\n\n        # Trim silence from beginning and end\n        trimmed_audio, _ = librosa.effects.trim(\n            audio_data,\n            top_db=20,  # Adjust threshold as needed\n            frame_length=2048,\n            hop_length=512\n        )\n\n        # Ensure minimum length for Whisper\n        min_samples = sample_rate * 0.1  # 0.1 seconds minimum\n        if len(trimmed_audio) < min_samples:\n            # Pad with zeros if too short\n            padding = min_samples - len(trimmed_audio)\n            trimmed_audio = np.pad(trimmed_audio, (0, padding), mode='constant')\n\n        return trimmed_audio\n\n    def detect_audio_quality_issues(self, audio_data, sample_rate=16000):\n        \"\"\"\n        Detect potential audio quality issues\n        \"\"\"\n        import numpy as np\n\n        metrics = {}\n\n        # Calculate RMS amplitude\n        rms = np.sqrt(np.mean(audio_data**2))\n        metrics['rms_amplitude'] = float(rms)\n\n        # Check for clipping\n        max_val = np.max(np.abs(audio_data))\n        metrics['clipping_ratio'] = float(max_val / 0.99) if max_val > 0 else 0.0\n\n        # Check for silence\n        metrics['is_silent'] = rms < self.silence_threshold\n\n        # Calculate signal-to-noise ratio (approximate)\n        if len(audio_data) > 1000:  # Need enough samples\n            # Estimate noise as the minimum amplitude in quiet segments\n            window_size = 1024\n            windows = len(audio_data) // window_size\n            window_amps = []\n\n            for i in range(windows):\n                window = audio_data[i*window_size:(i+1)*window_size]\n                window_amp = np.sqrt(np.mean(window**2))\n                window_amps.append(window_amp)\n\n            if window_amps:\n                noise_floor = np.percentile(window_amps, 10)  # 10th percentile as noise estimate\n                signal_level = np.percentile(window_amps, 90)  # 90th percentile as signal estimate\n                if noise_floor > 0:\n                    metrics['estimated_snr_db'] = 20 * np.log10(signal_level / noise_floor)\n                else:\n                    metrics['estimated_snr_db'] = float('inf')\n\n        return metrics\n"})}),"\n",(0,a.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(n.h3,{id:"example-1-basic-voice-command-system",children:"Example 1: Basic Voice Command System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Complete basic voice command system\nimport asyncio\nimport threading\nimport time\n\nclass BasicVoiceCommandSystem:\n    def __init__(self):\n        self.processor = WhisperVoiceProcessor(model_size="base")\n        self.validator = VoiceCommandValidator()\n        self.robot_control = VoiceControlNode()  # ROS 2 node\n\n        self.listening = False\n        self.command_queue = []\n\n    def start_listening(self):\n        """\n        Start the voice command listening system\n        """\n        self.listening = True\n\n        while self.listening:\n            try:\n                # Process a voice command\n                transcription = asyncio.run(\n                    self.processor.process_voice_command(timeout=5)\n                )\n\n                if transcription.strip():\n                    # Validate the command\n                    validation = self.validator.validate_command(transcription)\n\n                    if validation[\'is_valid\'] and validation[\'is_safe\']:\n                        # Add to command queue for processing\n                        self.command_queue.append({\n                            \'command\': transcription,\n                            \'validation\': validation,\n                            \'timestamp\': time.time()\n                        })\n\n                        # Execute immediately for this example\n                        self.execute_command(transcription, validation)\n                    else:\n                        print(f"Command rejected: {transcription}")\n\n            except Exception as e:\n                print(f"Error processing voice command: {e}")\n                time.sleep(0.1)  # Brief pause before retrying\n\n    def execute_command(self, command, validation):\n        """\n        Execute a validated voice command\n        """\n        print(f"Executing command: {command}")\n\n        # In a real system, you would send this to your robot control system\n        # For now, we\'ll just print what would happen\n        params = self.validator.extract_parameters(command, validation[\'command_type\'])\n        print(f"Command type: {validation[\'command_type\']}, Parameters: {params}")\n\n    def stop_listening(self):\n        """\n        Stop the voice command system\n        """\n        self.listening = False\n\n# Example usage\nif __name__ == "__main__":\n    system = BasicVoiceCommandSystem()\n\n    # Start listening in a separate thread\n    listen_thread = threading.Thread(target=system.start_listening)\n    listen_thread.start()\n\n    # Let it run for a while\n    time.sleep(30)  # Listen for 30 seconds\n\n    system.stop_listening()\n    listen_thread.join()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"example-2-voice-command-with-context-awareness",children:"Example 2: Voice Command with Context Awareness"}),"\n",(0,a.jsx)(n.p,{children:"Implementing voice commands that consider the robot's current state and environment."}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Poor Transcription Accuracy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Ensure audio is recorded at 16kHz sample rate"}),"\n",(0,a.jsx)(n.li,{children:"Use a good quality microphone positioned properly"}),"\n",(0,a.jsx)(n.li,{children:"Minimize background noise during recording"}),"\n",(0,a.jsx)(n.li,{children:"Consider using a larger Whisper model for better accuracy"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"High Latency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use smaller Whisper models for faster processing"}),"\n",(0,a.jsx)(n.li,{children:"Optimize audio preprocessing steps"}),"\n",(0,a.jsx)(n.li,{children:"Consider using GPU acceleration if available"}),"\n",(0,a.jsx)(n.li,{children:"Use streaming approaches for real-time processing"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Memory Issues"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use smaller models that fit in available memory"}),"\n",(0,a.jsx)(n.li,{children:"Process audio in smaller chunks"}),"\n",(0,a.jsx)(n.li,{children:"Clear model cache when switching models"}),"\n",(0,a.jsx)(n.li,{children:"Consider using the API for memory-intensive operations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Performance monitoring for voice processing\nclass VoiceProcessingMonitor:\n    def __init__(self):\n        self.transcription_times = []\n        self.accuracy_metrics = []\n        self.error_count = 0\n\n    def log_transcription(self, transcription_time, accuracy=None):\n        """\n        Log transcription performance metrics\n        """\n        self.transcription_times.append(transcription_time)\n        if accuracy is not None:\n            self.accuracy_metrics.append(accuracy)\n\n    def get_performance_summary(self):\n        """\n        Get summary of voice processing performance\n        """\n        if not self.transcription_times:\n            return "No data collected yet"\n\n        avg_time = sum(self.transcription_times) / len(self.transcription_times)\n        min_time = min(self.transcription_times)\n        max_time = max(self.transcription_times)\n\n        summary = {\n            \'average_transcription_time\': avg_time,\n            \'min_transcription_time\': min_time,\n            \'max_transcription_time\': max_time,\n            \'total_transcriptions\': len(self.transcription_times),\n            \'error_count\': self.error_count\n        }\n\n        if self.accuracy_metrics:\n            avg_accuracy = sum(self.accuracy_metrics) / len(self.accuracy_metrics)\n            summary[\'average_accuracy\'] = avg_accuracy\n\n        return summary\n'})}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Whisper Setup"}),": Install and configure Whisper with different model sizes, comparing performance and accuracy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Audio Processing"}),": Set up real-time audio capture and preprocessing pipeline for optimal Whisper performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Command Validation"}),": Implement a more sophisticated command validation system with custom command vocabularies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration Challenge"}),": Integrate voice processing with a simple robot simulation to execute basic commands"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter has introduced you to OpenAI Whisper for voice-to-action systems in humanoid robotics. You've learned how to set up and configure Whisper, process real-time audio, validate commands for safety, and integrate with robotic systems. The foundation laid here will be essential for the cognitive planning and complete VLA integration covered in subsequent chapters."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the next chapter, we'll explore cognitive planning using LLMs to convert natural language commands into ROS 2 actions, building upon the voice processing foundation established here."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>r});var s=i(6540);const a={},o=s.createContext(a);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);