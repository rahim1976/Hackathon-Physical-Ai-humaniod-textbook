"use strict";(globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend=globalThis.webpackChunkai_spec_driven_book_with_rag_chatbot_frontend||[]).push([[405],{4293(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vla-llm-integration/cognitive-planning-llms-ros2","title":"Cognitive Planning using LLMs for ROS 2","description":"Introduction","source":"@site/docs/vla-llm-integration/cognitive-planning-llms-ros2.md","sourceDirName":"vla-llm-integration","slug":"/vla-llm-integration/cognitive-planning-llms-ros2","permalink":"/docs/vla-llm-integration/cognitive-planning-llms-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-llm-integration/cognitive-planning-llms-ros2.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/docs/vla-llm-integration/voice-to-action-whisper"},"next":{"title":"Capstone: Autonomous Humanoid Executing Tasks","permalink":"/docs/vla-llm-integration/capstone-autonomous-humanoid-tasks"}}');var a=t(4848),s=t(8453);const o={},r="Cognitive Planning using LLMs for ROS 2",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding Cognitive Planning in Robotics",id:"understanding-cognitive-planning-in-robotics",level:2},{value:"The Cognitive Planning Problem",id:"the-cognitive-planning-problem",level:3},{value:"LLM Capabilities for Cognitive Planning",id:"llm-capabilities-for-cognitive-planning",level:3},{value:"Challenges in LLM-Based Planning",id:"challenges-in-llm-based-planning",level:3},{value:"LLM Selection and Setup",id:"llm-selection-and-setup",level:2},{value:"Popular LLM Options for Robotics",id:"popular-llm-options-for-robotics",level:3},{value:"Basic LLM Integration",id:"basic-llm-integration",level:3},{value:"Prompt Engineering for Cognitive Planning",id:"prompt-engineering-for-cognitive-planning",level:2},{value:"Effective Prompt Structure",id:"effective-prompt-structure",level:3},{value:"Advanced Prompt Techniques",id:"advanced-prompt-techniques",level:3},{value:"Mapping Natural Language to ROS 2 Actions",id:"mapping-natural-language-to-ros-2-actions",level:2},{value:"ROS 2 Action Structure",id:"ros-2-action-structure",level:3},{value:"Safety and Validation Layer",id:"safety-and-validation-layer",level:3},{value:"Context Management for Multi-Step Tasks",id:"context-management-for-multi-step-tasks",level:2},{value:"Maintaining Conversation Context",id:"maintaining-conversation-context",level:3},{value:"Task Decomposition and Planning",id:"task-decomposition-and-planning",level:3},{value:"Integration with ROS 2 Ecosystem",id:"integration-with-ros-2-ecosystem",level:2},{value:"ROS 2 Node Implementation",id:"ros-2-node-implementation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Optimization Strategies",id:"caching-and-optimization-strategies",level:3},{value:"Real-time Performance Considerations",id:"real-time-performance-considerations",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Simple Navigation Planning",id:"example-1-simple-navigation-planning",level:3},{value:"Example 2: Complex Multi-Step Task",id:"example-2-complex-multi-step-task",level:3},{value:"Troubleshooting and Best Practices",id:"troubleshooting-and-best-practices",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Monitoring and Evaluation",id:"monitoring-and-evaluation",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"cognitive-planning-using-llms-for-ros-2",children:"Cognitive Planning using LLMs for ROS 2"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"This chapter explores the use of Large Language Models (LLMs) for cognitive planning in robotics, specifically focusing on converting natural language commands into ROS 2 action sequences. Cognitive planning bridges the gap between high-level human instructions and low-level robotic actions, enabling natural human-robot interaction."}),"\n",(0,a.jsx)(e.p,{children:"The chapter will cover prompt engineering techniques for LLMs, mapping natural language to ROS 2 actions, implementing safety checks, and creating robust cognitive planning systems. You'll learn how to design LLM-based planners that can interpret complex commands and generate appropriate robot behaviors."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design effective prompts for LLM-based cognitive planning"}),"\n",(0,a.jsx)(e.li,{children:"Map natural language commands to ROS 2 action sequences"}),"\n",(0,a.jsx)(e.li,{children:"Implement safety checks and validation in cognitive planning"}),"\n",(0,a.jsx)(e.li,{children:"Create context-aware planning systems for multi-step tasks"}),"\n",(0,a.jsx)(e.li,{children:"Optimize LLM performance for real-time robotic applications"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(e.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Completed the voice-to-action chapter (Module 4, Chapter 1)"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 action concepts and message types"}),"\n",(0,a.jsx)(e.li,{children:"Basic knowledge of LLM APIs (OpenAI, Anthropic, etc.)"}),"\n",(0,a.jsx)(e.li,{children:"Familiarity with natural language processing concepts"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"understanding-cognitive-planning-in-robotics",children:"Understanding Cognitive Planning in Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"the-cognitive-planning-problem",children:"The Cognitive Planning Problem"}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning in robotics involves translating high-level goals or natural language commands into executable action sequences. This process requires:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Understanding"}),": Interpreting the user's intent from natural language"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reasoning"}),": Determining the appropriate sequence of actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Planning"}),": Creating a detailed plan of ROS 2 actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Execution"}),": Coordinating the execution of the planned actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitoring"}),": Tracking execution and adapting as needed"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"llm-capabilities-for-cognitive-planning",children:"LLM Capabilities for Cognitive Planning"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models excel at cognitive planning tasks because they can:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand complex natural language commands"}),"\n",(0,a.jsx)(e.li,{children:"Reason about object relationships and spatial concepts"}),"\n",(0,a.jsx)(e.li,{children:"Generate structured outputs (JSON, XML) that can be parsed"}),"\n",(0,a.jsx)(e.li,{children:"Handle multi-step reasoning and planning"}),"\n",(0,a.jsx)(e.li,{children:"Adapt to new situations through few-shot learning"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"challenges-in-llm-based-planning",children:"Challenges in LLM-Based Planning"}),"\n",(0,a.jsx)(e.p,{children:"However, LLMs also present challenges for robotic applications:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Non-deterministic outputs"}),": LLMs may generate inconsistent responses"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Lack of real-time constraints"}),": LLMs may not consider timing constraints"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety concerns"}),": LLMs may generate unsafe commands without proper validation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context limitations"}),": LLMs have finite context windows"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Latency"}),": LLM calls may introduce significant delays"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"llm-selection-and-setup",children:"LLM Selection and Setup"}),"\n",(0,a.jsx)(e.h3,{id:"popular-llm-options-for-robotics",children:"Popular LLM Options for Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Different LLMs offer various trade-offs for cognitive planning:"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"LLM"}),(0,a.jsx)(e.th,{children:"Strengths"}),(0,a.jsx)(e.th,{children:"Weaknesses"}),(0,a.jsx)(e.th,{children:"Use Case"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"GPT-4"}),(0,a.jsx)(e.td,{children:"High accuracy, good reasoning"}),(0,a.jsx)(e.td,{children:"Expensive, slower"}),(0,a.jsx)(e.td,{children:"Complex planning tasks"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"GPT-3.5"}),(0,a.jsx)(e.td,{children:"Good balance, faster"}),(0,a.jsx)(e.td,{children:"Less reasoning capability"}),(0,a.jsx)(e.td,{children:"Real-time applications"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Claude"}),(0,a.jsx)(e.td,{children:"Good reasoning, longer context"}),(0,a.jsx)(e.td,{children:"Newer, limited access"}),(0,a.jsx)(e.td,{children:"Complex multi-step planning"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Local models (Llama, etc.)"}),(0,a.jsx)(e.td,{children:"Private, controllable"}),(0,a.jsx)(e.td,{children:"Requires significant resources"}),(0,a.jsx)(e.td,{children:"Privacy-sensitive applications"})]})]})]}),"\n",(0,a.jsx)(e.h3,{id:"basic-llm-integration",children:"Basic LLM Integration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Basic LLM integration for cognitive planning\nimport openai\nimport json\nimport os\nfrom typing import Dict, List, Any\n\nclass LLMBridge:\n    def __init__(self, api_key: str = None, model: str = "gpt-3.5-turbo"):\n        if api_key:\n            openai.api_key = api_key\n        else:\n            openai.api_key = os.getenv("OPENAI_API_KEY")\n\n        self.model = model\n\n    def call_llm(self, prompt: str, max_tokens: int = 500, temperature: float = 0.3) -> str:\n        """\n        Call the LLM with a given prompt\n        """\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}],\n            max_tokens=max_tokens,\n            temperature=temperature,\n            response_format={"type": "json_object"}  # For structured output\n        )\n\n        return response.choices[0].message[\'content\'].strip()\n\n    def validate_response(self, response: str) -> Dict[str, Any]:\n        """\n        Validate and parse LLM response\n        """\n        try:\n            parsed = json.loads(response)\n            return parsed\n        except json.JSONDecodeError:\n            # If JSON parsing fails, try to extract structured data using regex\n            # This is a fallback approach\n            return {"error": "Invalid JSON response", "raw_response": response}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"prompt-engineering-for-cognitive-planning",children:"Prompt Engineering for Cognitive Planning"}),"\n",(0,a.jsx)(e.h3,{id:"effective-prompt-structure",children:"Effective Prompt Structure"}),"\n",(0,a.jsx)(e.p,{children:"Well-designed prompts are crucial for reliable cognitive planning:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Structured prompt for cognitive planning\nclass CognitivePlanningPrompter:\n    def __init__(self):\n        self.system_prompt = """\n        You are a cognitive planning assistant for a humanoid robot. Your task is to interpret natural language commands and convert them into structured action sequences for ROS 2 execution.\n\n        Guidelines:\n        1. Always prioritize safety - never generate commands that could harm humans or damage property\n        2. Break complex commands into simple, executable steps\n        3. Use only the action types provided in the action vocabulary\n        4. Include appropriate parameters for each action\n        5. Consider the robot\'s current state and environment when planning\n\n        Action Vocabulary:\n        - navigation: move to a location, parameters: [x, y, theta, frame_id]\n        - manipulation: manipulate objects, parameters: [object_name, action_type, grasp_pose]\n        - perception: perceive environment, parameters: [sensor_type, target_object]\n        - communication: communicate with humans, parameters: [message_type, content]\n\n        Output Format:\n        {\n            "command": "original user command",\n            "intent": "parsed intent",\n            "actions": [\n                {\n                    "action_type": "navigation|manipulation|perception|communication",\n                    "parameters": {"param1": "value1", ...},\n                    "description": "human-readable description"\n                }\n            ],\n            "confidence": 0.0-1.0,\n            "safety_check_passed": true/false\n        }\n        """\n\n    def create_planning_prompt(self, user_command: str, robot_state: Dict = None,\n                              environment_info: Dict = None) -> str:\n        """\n        Create a structured prompt for cognitive planning\n        """\n        prompt = f"{self.system_prompt}\\n\\n"\n\n        if robot_state:\n            prompt += f"Current Robot State: {json.dumps(robot_state)}\\n\\n"\n\n        if environment_info:\n            prompt += f"Environment Information: {json.dumps(environment_info)}\\n\\n"\n\n        prompt += f"User Command: {user_command}\\n\\n"\n        prompt += "Please generate the appropriate action sequence:"\n\n        return prompt\n\n    def create_validation_prompt(self, action_sequence: List[Dict], user_command: str) -> str:\n        """\n        Create a prompt to validate an action sequence\n        """\n        validation_prompt = f"""\n        You are validating a robot action sequence for safety and correctness.\n\n        Original Command: {user_command}\n\n        Action Sequence: {json.dumps(action_sequence, indent=2)}\n\n        Please validate this sequence and respond in JSON format:\n        {{\n            "is_safe": true/false,\n            "is_complete": true/false,\n            "issues": ["list of issues if any"],\n            "suggestions": ["list of suggestions if needed"]\n        }}\n        """\n\n        return validation_prompt\n'})}),"\n",(0,a.jsx)(e.h3,{id:"advanced-prompt-techniques",children:"Advanced Prompt Techniques"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Advanced prompt engineering techniques\nclass AdvancedPrompter:\n    def __init__(self):\n        self.action_templates = {\n            "navigation": {\n                "examples": [\n                    {\n                        "command": "Go to the kitchen",\n                        "actions": [{"action_type": "navigation", "parameters": {"x": 2.5, "y": 1.0, "theta": 0.0, "frame_id": "map"}}]\n                    },\n                    {\n                        "command": "Move 2 meters forward",\n                        "actions": [{"action_type": "navigation", "parameters": {"x": 2.0, "y": 0.0, "theta": 0.0, "frame_id": "base_link"}}]\n                    }\n                ]\n            },\n            "manipulation": {\n                "examples": [\n                    {\n                        "command": "Pick up the red cup",\n                        "actions": [{"action_type": "perception", "parameters": {"sensor_type": "camera", "target_object": "red cup"}},\n                                   {"action_type": "manipulation", "parameters": {"object_name": "red cup", "action_type": "grasp", "grasp_pose": {"x": 0.5, "y": 0.2, "z": 0.1}}}]\n                    }\n                ]\n            }\n        }\n\n    def few_shot_prompt(self, user_command: str, command_type: str) -> str:\n        """\n        Create a few-shot learning prompt with examples\n        """\n        examples = self.action_templates.get(command_type, {}).get("examples", [])\n\n        prompt = f"""\n        You are a cognitive planning assistant. Here are examples of how to convert natural language commands to action sequences:\n\n        Examples:\n        """\n\n        for i, example in enumerate(examples[:2]):  # Use first 2 examples\n            prompt += f"\\nExample {i+1}:"\n            prompt += f"\\nCommand: {example[\'command\']}"\n            prompt += f"\\nActions: {json.dumps(example[\'actions\'], indent=2)}"\n            prompt += "\\n---"\n\n        prompt += f"\\n\\nNow process this command: {user_command}"\n        prompt += "\\n\\nRespond with the appropriate action sequence in JSON format."\n\n        return prompt\n\n    def chain_of_thought_prompt(self, user_command: str) -> str:\n        """\n        Create a chain-of-thought prompt that explains the reasoning\n        """\n        prompt = f"""\n        You are a cognitive planning assistant. For the following command, think step by step:\n\n        Command: {user_command}\n\n        Step-by-step reasoning:\n        1. What is the user trying to achieve?\n        2. What are the key objects or locations involved?\n        3. What sequence of actions is needed?\n        4. What parameters are required for each action?\n\n        Finally, provide the action sequence in JSON format with the following structure:\n        {{\n            "command": "{user_command}",\n            "reasoning": "step-by-step explanation",\n            "actions": [\n                {{\n                    "action_type": "navigation|manipulation|perception|communication",\n                    "parameters": {{"param1": "value1", ...}},\n                    "description": "what this action does"\n                }}\n            ]\n        }}\n        """\n\n        return prompt\n'})}),"\n",(0,a.jsx)(e.h2,{id:"mapping-natural-language-to-ros-2-actions",children:"Mapping Natural Language to ROS 2 Actions"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-action-structure",children:"ROS 2 Action Structure"}),"\n",(0,a.jsx)(e.p,{children:"Understanding how to map LLM outputs to ROS 2 actions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Example: ROS 2 action mapping system\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Point\nfrom std_msgs.msg import String\n\nclass ROS2ActionMapper(Node):\n    def __init__(self):\n        super().__init__('ros2_action_mapper')\n\n        # Action clients for different robot capabilities\n        self.navigation_client = ActionClient(self, 'nav2_msgs/action/NavigateToPose', 'navigate_to_pose')\n        self.manipulation_client = ActionClient(self, 'manipulation_msgs/action/Grasp', 'grasp_object')\n\n        # Publishers for other actions\n        self.cmd_vel_publisher = self.create_publisher('geometry_msgs/msg/Twist', 'cmd_vel', 10)\n        self.speech_publisher = self.create_publisher('std_msgs/msg/String', 'robot_speech', 10)\n\n    def execute_action_sequence(self, action_sequence: List[Dict]) -> bool:\n        \"\"\"\n        Execute a sequence of actions mapped from LLM output\n        \"\"\"\n        success = True\n\n        for action in action_sequence:\n            action_type = action.get('action_type')\n            parameters = action.get('parameters', {})\n\n            try:\n                if action_type == 'navigation':\n                    success &= self.execute_navigation_action(parameters)\n                elif action_type == 'manipulation':\n                    success &= self.execute_manipulation_action(parameters)\n                elif action_type == 'perception':\n                    success &= self.execute_perception_action(parameters)\n                elif action_type == 'communication':\n                    success &= self.execute_communication_action(parameters)\n                else:\n                    self.get_logger().warn(f\"Unknown action type: {action_type}\")\n                    success = False\n\n            except Exception as e:\n                self.get_logger().error(f\"Error executing action {action_type}: {e}\")\n                success = False\n\n        return success\n\n    def execute_navigation_action(self, params: Dict) -> bool:\n        \"\"\"\n        Execute navigation action\n        \"\"\"\n        from nav2_msgs.action import NavigateToPose\n        from geometry_msgs.msg import PoseStamped\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = params.get('frame_id', 'map')\n        goal_msg.pose.pose.position.x = params.get('x', 0.0)\n        goal_msg.pose.pose.position.y = params.get('y', 0.0)\n\n        # Set orientation\n        import math\n        theta = params.get('theta', 0.0)\n        goal_msg.pose.pose.orientation.z = math.sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = math.cos(theta / 2.0)\n\n        # Send goal\n        self.navigation_client.wait_for_server()\n        future = self.navigation_client.send_goal_async(goal_msg)\n\n        # Wait for result (in a real system, you'd handle this asynchronously)\n        # rclpy.spin_until_future_complete(self, future)\n\n        return True  # Simplified for example\n\n    def execute_manipulation_action(self, params: Dict) -> bool:\n        \"\"\"\n        Execute manipulation action\n        \"\"\"\n        # Implementation would depend on your specific manipulation stack\n        object_name = params.get('object_name')\n        action_type = params.get('action_type')\n\n        self.get_logger().info(f\"Manipulation: {action_type} {object_name}\")\n        return True\n\n    def execute_perception_action(self, params: Dict) -> bool:\n        \"\"\"\n        Execute perception action\n        \"\"\"\n        sensor_type = params.get('sensor_type')\n        target_object = params.get('target_object')\n\n        self.get_logger().info(f\"Perception: {sensor_type} for {target_object}\")\n        return True\n\n    def execute_communication_action(self, params: Dict) -> bool:\n        \"\"\"\n        Execute communication action\n        \"\"\"\n        message_type = params.get('message_type')\n        content = params.get('content')\n\n        msg = String()\n        msg.data = content\n        self.speech_publisher.publish(msg)\n\n        return True\n"})}),"\n",(0,a.jsx)(e.h3,{id:"safety-and-validation-layer",children:"Safety and Validation Layer"}),"\n",(0,a.jsx)(e.p,{children:"Implementing safety checks for LLM-generated actions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Example: Safety and validation layer\nclass SafetyValidator:\n    def __init__(self):\n        self.safety_rules = [\n            self.no_self_harm,\n            self.no_harm_to_others,\n            self.no_property_damage,\n            self.no_impossible_actions,\n            self.no_privacy_violations\n        ]\n\n        # Known dangerous commands\n        self.dangerous_keywords = [\n            'self-destruct', 'harm', 'destroy', 'damage', 'kill', 'attack',\n            'violate', 'break', 'ignore safety', 'emergency stop'\n        ]\n\n    def validate_action_sequence(self, action_sequence: List[Dict],\n                               original_command: str = \"\") -> Dict[str, Any]:\n        \"\"\"\n        Validate an action sequence for safety\n        \"\"\"\n        validation_result = {\n            'is_safe': True,\n            'issues': [],\n            'suggestions': []\n        }\n\n        # Check for dangerous keywords in original command\n        for keyword in self.dangerous_keywords:\n            if keyword.lower() in original_command.lower():\n                validation_result['is_safe'] = False\n                validation_result['issues'].append(f\"Dangerous keyword detected: {keyword}\")\n                return validation_result\n\n        # Validate each action\n        for i, action in enumerate(action_sequence):\n            action_validation = self.validate_single_action(action, i)\n\n            if not action_validation['is_safe']:\n                validation_result['is_safe'] = False\n                validation_result['issues'].extend(action_validation['issues'])\n\n        return validation_result\n\n    def validate_single_action(self, action: Dict, index: int) -> Dict[str, Any]:\n        \"\"\"\n        Validate a single action for safety\n        \"\"\"\n        result = {'is_safe': True, 'issues': [], 'suggestions': []}\n\n        action_type = action.get('action_type', '')\n        params = action.get('parameters', {})\n\n        # Apply safety rules\n        for rule in self.safety_rules:\n            rule_result = rule(action_type, params, index)\n            if not rule_result['is_safe']:\n                result['is_safe'] = False\n                result['issues'].extend(rule_result['issues'])\n\n        return result\n\n    def no_self_harm(self, action_type: str, params: Dict, index: int) -> Dict[str, Any]:\n        \"\"\"\n        Rule: Prevent actions that could harm the robot\n        \"\"\"\n        issues = []\n\n        # Check for commands that might damage the robot\n        if action_type == 'navigation':\n            # Check for extreme velocities\n            vel_x = params.get('linear_velocity', 0)\n            if abs(vel_x) > 2.0:  # Adjust threshold as needed\n                issues.append(f\"Navigation action {index}: Excessive linear velocity ({vel_x})\")\n\n        return {'is_safe': len(issues) == 0, 'issues': issues}\n\n    def no_harm_to_others(self, action_type: str, params: Dict, index: int) -> Dict[str, Any]:\n        \"\"\"\n        Rule: Prevent actions that could harm humans\n        \"\"\"\n        issues = []\n\n        # Check for potentially dangerous navigation\n        if action_type == 'navigation':\n            # Check for navigation toward humans (simplified check)\n            target_x = params.get('x', 0)\n            target_y = params.get('y', 0)\n            target_distance = (target_x**2 + target_y**2)**0.5\n\n            if target_distance < 0.5:  # Too close to target\n                issues.append(f\"Navigation action {index}: Target too close ({target_distance:.2f}m)\")\n\n        return {'is_safe': len(issues) == 0, 'issues': issues}\n\n    def no_impossible_actions(self, action_type: str, params: Dict, index: int) -> Dict[str, Any]:\n        \"\"\"\n        Rule: Prevent physically impossible actions\n        \"\"\"\n        issues = []\n\n        # Check for impossible manipulation\n        if action_type == 'manipulation':\n            object_weight = params.get('object_weight', 0)\n            if object_weight > 10.0:  # Assuming robot can't lift more than 10kg\n                issues.append(f\"Manipulation action {index}: Object too heavy ({object_weight}kg)\")\n\n        return {'is_safe': len(issues) == 0, 'issues': issues}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"context-management-for-multi-step-tasks",children:"Context Management for Multi-Step Tasks"}),"\n",(0,a.jsx)(e.h3,{id:"maintaining-conversation-context",children:"Maintaining Conversation Context"}),"\n",(0,a.jsx)(e.p,{children:"Handling multi-step tasks that require maintaining context:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Context management for multi-step tasks\nclass ContextManager:\n    def __init__(self):\n        self.conversation_history = []\n        self.robot_state = {}\n        self.environment_map = {}\n        self.object_locations = {}\n        self.task_context = {}\n\n    def update_context(self, user_command: str, action_sequence: List[Dict],\n                      execution_result: Dict = None):\n        """\n        Update the context with new information\n        """\n        # Add to conversation history\n        self.conversation_history.append({\n            \'user_command\': user_command,\n            \'action_sequence\': action_sequence,\n            \'execution_result\': execution_result,\n            \'timestamp\': time.time()\n        })\n\n        # Keep only recent history to manage context window\n        if len(self.conversation_history) > 10:  # Keep last 10 interactions\n            self.conversation_history = self.conversation_history[-10:]\n\n    def get_context_prompt(self) -> str:\n        """\n        Get a prompt with relevant context information\n        """\n        context_prompt = "Context Information:\\n"\n\n        # Add recent conversation history\n        if self.conversation_history:\n            context_prompt += "Recent interactions:\\n"\n            for interaction in self.conversation_history[-3:]:  # Last 3 interactions\n                context_prompt += f"- User: {interaction[\'user_command\']}\\n"\n                if interaction[\'execution_result\']:\n                    context_prompt += f"  Result: {interaction[\'execution_result\'].get(\'status\', \'unknown\')}\\n"\n\n        # Add current robot state\n        if self.robot_state:\n            context_prompt += f"Robot state: {json.dumps(self.robot_state)}\\n"\n\n        # Add known object locations\n        if self.object_locations:\n            context_prompt += f"Known objects: {json.dumps(self.object_locations)}\\n"\n\n        return context_prompt\n\n    def resolve_pronouns(self, command: str) -> str:\n        """\n        Resolve pronouns in the command based on context\n        """\n        # Simple pronoun resolution based on context\n        if "it" in command.lower() and self.object_locations:\n            # Replace "it" with the most recently mentioned object\n            last_object = list(self.object_locations.keys())[-1] if self.object_locations else None\n            if last_object:\n                command = command.replace(" it ", f" {last_object} ")\n\n        if "there" in command.lower() and self.conversation_history:\n            # Replace "there" with the last mentioned location\n            last_action = self.conversation_history[-1].get(\'action_sequence\', [{}])[-1]\n            if last_action.get(\'action_type\') == \'navigation\':\n                params = last_action.get(\'parameters\', {})\n                location = f"{params.get(\'x\', 0)}, {params.get(\'y\', 0)}"\n                command = command.replace(" there ", f" to {location} ")\n\n        return command\n'})}),"\n",(0,a.jsx)(e.h3,{id:"task-decomposition-and-planning",children:"Task Decomposition and Planning"}),"\n",(0,a.jsx)(e.p,{children:"Breaking down complex tasks into manageable steps:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Task decomposition and hierarchical planning\nclass TaskDecomposer:\n    def __init__(self):\n        self.task_library = {\n            "fetch_object": ["locate_object", "navigate_to_object", "grasp_object", "return_with_object"],\n            "room_cleaning": ["survey_room", "identify_objects", "categorize_objects", "move_objects"],\n            "guided_tour": ["welcome_user", "navigate_to_point", "provide_information", "proceed_to_next_point"]\n        }\n\n    def decompose_task(self, high_level_command: str) -> List[str]:\n        """\n        Decompose a high-level command into subtasks\n        """\n        # Check if command matches a known task pattern\n        command_lower = high_level_command.lower()\n\n        for task_name, subtasks in self.task_library.items():\n            if task_name in command_lower:\n                return subtasks\n\n        # If no known pattern, return the command as a single task\n        return [high_level_command]\n\n    def create_hierarchical_plan(self, high_level_command: str,\n                               llm_bridge: LLMBridge) -> List[Dict]:\n        """\n        Create a hierarchical plan for complex tasks\n        """\n        subtasks = self.decompose_task(high_level_command)\n        hierarchical_plan = []\n\n        for subtask in subtasks:\n            # Generate detailed plan for each subtask\n            prompt = f"Create a detailed action plan for: {subtask}. " \\\n                    f"Original high-level command: {high_level_command}. " \\\n                    "Respond in JSON format with the action sequence."\n\n            response = llm_bridge.call_llm(prompt)\n            subtask_plan = llm_bridge.validate_response(response)\n\n            if "actions" in subtask_plan:\n                hierarchical_plan.append({\n                    "subtask": subtask,\n                    "actions": subtask_plan["actions"],\n                    "description": subtask_plan.get("intent", subtask)\n                })\n            else:\n                hierarchical_plan.append({\n                    "subtask": subtask,\n                    "actions": [{"action_type": "communication",\n                               "parameters": {"message_type": "error",\n                                            "content": f"Could not plan subtask: {subtask}"}}],\n                    "description": f"Failed to plan subtask: {subtask}"\n                })\n\n        return hierarchical_plan\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-ros-2-ecosystem",children:"Integration with ROS 2 Ecosystem"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-node-implementation",children:"ROS 2 Node Implementation"}),"\n",(0,a.jsx)(e.p,{children:"Creating a complete ROS 2 node for cognitive planning:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Complete ROS 2 cognitive planning node\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import PoseStamped\nimport asyncio\nimport threading\nimport time\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planner_node\')\n\n        # Initialize components\n        self.llm_bridge = LLMBridge()\n        self.prompter = CognitivePlanningPrompter()\n        self.action_mapper = ROS2ActionMapper()\n        self.safety_validator = SafetyValidator()\n        self.context_manager = ContextManager()\n        self.task_decomposer = TaskDecomposer()\n\n        # Subscribers\n        self.voice_command_sub = self.create_subscription(\n            String, \'voice_commands\', self.voice_command_callback, 10\n        )\n        self.text_command_sub = self.create_subscription(\n            String, \'text_commands\', self.text_command_callback, 10\n        )\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, \'cognitive_planner_status\', 10)\n        self.action_sequence_pub = self.create_publisher(String, \'action_sequence\', 10)\n\n        # Service clients for environment information\n        self.get_map_client = self.create_client(GetMap, \'map_server/map\')\n\n        self.get_logger().info(\'Cognitive Planner Node initialized\')\n\n    def voice_command_callback(self, msg):\n        """\n        Handle voice command input\n        """\n        self.process_command(msg.data, command_type=\'voice\')\n\n    def text_command_callback(self, msg):\n        """\n        Handle text command input\n        """\n        self.process_command(msg.data, command_type=\'text\')\n\n    def process_command(self, command: str, command_type: str = \'text\'):\n        """\n        Process a natural language command through the cognitive planning pipeline\n        """\n        try:\n            # Resolve pronouns and context\n            resolved_command = self.context_manager.resolve_pronouns(command)\n\n            # Get environment information\n            env_info = self.get_environment_info()\n\n            # Create planning prompt with context\n            context_prompt = self.context_manager.get_context_prompt()\n            full_prompt = self.prompter.create_planning_prompt(\n                resolved_command,\n                robot_state=self.get_robot_state(),\n                environment_info=env_info\n            )\n\n            # Call LLM to generate action sequence\n            llm_response = self.llm_bridge.call_llm(full_prompt)\n            action_sequence = self.llm_bridge.validate_response(llm_response)\n\n            # Validate for safety\n            validation_result = self.safety_validator.validate_action_sequence(\n                action_sequence.get(\'actions\', []), resolved_command\n            )\n\n            if not validation_result[\'is_safe\']:\n                self.get_logger().error(f"Unsafe action sequence rejected: {validation_result[\'issues\']}")\n                self.publish_status(f"Command rejected for safety reasons: {validation_result[\'issues\']}")\n                return\n\n            # Publish action sequence for execution\n            action_msg = String()\n            action_msg.data = json.dumps(action_sequence)\n            self.action_sequence_pub.publish(action_msg)\n\n            # Update context with this interaction\n            self.context_manager.update_context(resolved_command, action_sequence)\n\n            self.get_logger().info(f"Generated action sequence for: {resolved_command}")\n            self.publish_status(f"Planning complete for: {resolved_command}")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing command \'{command}\': {e}")\n            self.publish_status(f"Error processing command: {e}")\n\n    def get_environment_info(self) -> Dict:\n        """\n        Get current environment information\n        """\n        env_info = {}\n\n        # This would integrate with perception systems to get:\n        # - Object locations\n        # - Obstacle maps\n        # - Human locations\n        # - Room layout\n\n        # For now, return a simple representation\n        return {\n            "object_locations": self.context_manager.object_locations,\n            "known_rooms": ["kitchen", "living_room", "bedroom"],\n            "robot_location": self.get_robot_position()\n        }\n\n    def get_robot_state(self) -> Dict:\n        """\n        Get current robot state\n        """\n        # This would integrate with robot state publisher\n        return {\n            "battery_level": 0.85,\n            "current_pose": self.get_robot_position(),\n            "manipulator_status": "ready",\n            "navigation_status": "idle"\n        }\n\n    def get_robot_position(self) -> Dict:\n        """\n        Get robot\'s current position (simplified)\n        """\n        # In a real system, this would come from TF or localization\n        return {"x": 0.0, "y": 0.0, "theta": 0.0, "frame_id": "map"}\n\n    def publish_status(self, status: str):\n        """\n        Publish status message\n        """\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"caching-and-optimization-strategies",children:"Caching and Optimization Strategies"}),"\n",(0,a.jsx)(e.p,{children:"Optimizing LLM-based cognitive planning for performance:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Performance optimization strategies\nimport functools\nimport hashlib\nfrom typing import Optional\n\nclass OptimizedCognitivePlanner:\n    def __init__(self, llm_bridge: LLMBridge):\n        self.llm_bridge = llm_bridge\n        self.response_cache = {}\n        self.cache_size_limit = 100\n\n    def get_cache_key(self, prompt: str, params: Dict) -> str:\n        """\n        Generate a cache key for the prompt and parameters\n        """\n        cache_input = f"{prompt}_{json.dumps(params, sort_keys=True)}"\n        return hashlib.md5(cache_input.encode()).hexdigest()\n\n    @functools.lru_cache(maxsize=50)\n    def cached_llm_call(self, prompt: str, max_tokens: int = 500) -> str:\n        """\n        Cached LLM call for frequently used prompts\n        """\n        return self.llm_bridge.call_llm(prompt, max_tokens=max_tokens)\n\n    def intelligent_caching(self, command: str, action_sequence: List[Dict]):\n        """\n        Implement intelligent caching based on command similarity\n        """\n        # Cache simple, frequently used commands\n        if len(action_sequence) <= 3:  # Simple commands\n            cache_key = hashlib.md5(command.encode()).hexdigest()\n            self.response_cache[cache_key] = action_sequence\n\n            # Limit cache size\n            if len(self.response_cache) > self.cache_size_limit:\n                # Remove oldest entries (simplified)\n                oldest_key = next(iter(self.response_cache))\n                del self.response_cache[oldest_key]\n\n    def batch_process_commands(self, commands: List[str]) -> List[Dict]:\n        """\n        Batch process multiple commands for efficiency\n        """\n        # In a real implementation, this would send multiple commands\n        # to the LLM in a single request\n        results = []\n\n        for command in commands:\n            # Check cache first\n            cache_key = hashlib.md5(command.encode()).hexdigest()\n            if cache_key in self.response_cache:\n                results.append(self.response_cache[cache_key])\n            else:\n                # Process normally\n                result = self.process_single_command(command)\n                results.append(result)\n\n                # Cache if appropriate\n                self.intelligent_caching(command, result)\n\n        return results\n\n    def adaptive_prompting(self, command_complexity: str) -> str:\n        """\n        Use different prompting strategies based on command complexity\n        """\n        if command_complexity == "simple":\n            return "simple_direct"\n        elif command_complexity == "moderate":\n            return "few_shot"\n        else:  # complex\n            return "chain_of_thought"\n'})}),"\n",(0,a.jsx)(e.h3,{id:"real-time-performance-considerations",children:"Real-time Performance Considerations"}),"\n",(0,a.jsx)(e.p,{children:"Optimizing for real-time robotic applications:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Real-time performance optimization\nclass RealTimeCognitivePlanner:\n    def __init__(self, llm_bridge: LLMBridge):\n        self.llm_bridge = llm_bridge\n        self.max_response_time = 2.0  # Maximum time to wait for LLM response\n        self.fallback_planner = SimpleFallbackPlanner()  # Fallback for time-critical tasks\n\n    def process_with_timeout(self, prompt: str, timeout: float = 2.0) -> Optional[Dict]:\n        """\n        Process command with timeout to ensure real-time performance\n        """\n        import concurrent.futures\n\n        # Submit LLM call to thread pool with timeout\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future = executor.submit(self.llm_bridge.call_llm, prompt)\n\n            try:\n                result = future.result(timeout=timeout)\n                return self.llm_bridge.validate_response(result)\n            except concurrent.futures.TimeoutError:\n                self.get_logger().warn("LLM call timed out, using fallback")\n                return self.fallback_planner.process(prompt)\n\n    def predictive_planning(self, context: Dict) -> List[Dict]:\n        """\n        Pre-generate likely action sequences based on context\n        """\n        # In high-level applications, predict likely next commands\n        # and pre-generate action sequences\n        predicted_commands = self.predict_next_commands(context)\n        precomputed_actions = {}\n\n        for cmd in predicted_commands:\n            action_seq = self.process_with_timeout(\n                self.prompter.create_planning_prompt(cmd, context),\n                timeout=1.0  # Shorter timeout for predictions\n            )\n            precomputed_actions[cmd] = action_seq\n\n        return precomputed_actions\n'})}),"\n",(0,a.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-simple-navigation-planning",children:"Example 1: Simple Navigation Planning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Simple navigation command planning\ndef example_simple_navigation():\n    """\n    Example of planning a simple navigation command\n    """\n    llm_bridge = LLMBridge()\n    prompter = CognitivePlanningPrompter()\n    safety_validator = SafetyValidator()\n\n    # User command\n    command = "Please go to the kitchen"\n\n    # Create planning prompt\n    prompt = prompter.create_planning_prompt(\n        command,\n        robot_state={"x": 0.0, "y": 0.0, "theta": 0.0, "location": "living_room"},\n        environment_info={"kitchen_location": {"x": 5.0, "y": 3.0}}\n    )\n\n    # Get LLM response\n    response = llm_bridge.call_llm(prompt)\n    action_sequence = llm_bridge.validate_response(response)\n\n    print(f"Command: {command}")\n    print(f"Generated actions: {json.dumps(action_sequence, indent=2)}")\n\n    # Validate for safety\n    validation = safety_validator.validate_action_sequence(\n        action_sequence.get(\'actions\', []), command\n    )\n    print(f"Safety validation: {validation}")\n\n    return action_sequence\n\n# Run the example\nnavigation_plan = example_simple_navigation()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-complex-multi-step-task",children:"Example 2: Complex Multi-Step Task"}),"\n",(0,a.jsx)(e.p,{children:"Implementing a complex task that requires multiple cognitive planning steps."}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting-and-best-practices",children:"Troubleshooting and Best Practices"}),"\n",(0,a.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"LLM Inconsistency"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use structured output format (JSON) to ensure consistent parsing"}),"\n",(0,a.jsx)(e.li,{children:"Implement validation layers to check for expected output structure"}),"\n",(0,a.jsx)(e.li,{children:"Use temperature < 0.5 for more deterministic outputs"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Performance Bottlenecks"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement caching for frequently used commands"}),"\n",(0,a.jsx)(e.li,{children:"Use smaller models for simple tasks, larger models for complex reasoning"}),"\n",(0,a.jsx)(e.li,{children:"Consider local models for privacy and performance"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Safety Concerns"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Always implement multiple layers of safety validation"}),"\n",(0,a.jsx)(e.li,{children:"Use human-in-the-loop for critical operations"}),"\n",(0,a.jsx)(e.li,{children:"Maintain fallback mechanisms for safety-critical scenarios"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"monitoring-and-evaluation",children:"Monitoring and Evaluation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Example: Monitoring and evaluation for cognitive planning\nclass CognitivePlanningMonitor:\n    def __init__(self):\n        self.planning_times = []\n        self.success_rates = []\n        self.safety_violations = []\n        self.user_satisfaction = []\n\n    def log_planning_event(self, command: str, action_sequence: List[Dict],\n                          planning_time: float, success: bool):\n        """\n        Log a cognitive planning event for monitoring\n        """\n        self.planning_times.append(planning_time)\n\n        if success:\n            self.success_rates.append(1)\n        else:\n            self.success_rates.append(0)\n\n    def evaluate_planning_quality(self, original_command: str,\n                                generated_actions: List[Dict]) -> Dict:\n        """\n        Evaluate the quality of generated action sequences\n        """\n        metrics = {\n            \'completeness\': 0.0,  # How well the plan addresses the command\n            \'efficiency\': 0.0,    # How efficient the action sequence is\n            \'safety\': 0.0,        # Safety rating of the plan\n            \'naturalness\': 0.0    # How natural the plan seems\n        }\n\n        # Calculate metrics based on various factors\n        # This is a simplified example\n        if generated_actions:\n            metrics[\'completeness\'] = min(1.0, len(generated_actions) / 10)  # Arbitrary scale\n            metrics[\'efficiency\'] = 1.0 / len(generated_actions)  # Simpler is more efficient\n            metrics[\'naturalness\'] = 0.8  # Default assumption\n\n        return metrics\n\n    def get_performance_report(self) -> str:\n        """\n        Generate a performance report\n        """\n        if not self.planning_times:\n            return "No data collected yet"\n\n        avg_time = sum(self.planning_times) / len(self.planning_times)\n        success_rate = sum(self.success_rates) / len(self.success_rates) if self.success_rates else 0\n\n        report = f"""\n        Cognitive Planning Performance Report:\n        - Average planning time: {avg_time:.2f}s\n        - Success rate: {success_rate:.2%}\n        - Total plans generated: {len(self.planning_times)}\n        - Safety violations: {len(self.safety_violations)}\n        """\n\n        return report\n'})}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Prompt Engineering"}),": Design and test different prompt structures to optimize LLM performance for cognitive planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Validation"}),": Implement additional safety rules and validation mechanisms for robotic action sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context Management"}),": Create a more sophisticated context management system that handles complex multi-turn interactions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Performance Optimization"}),": Implement caching and optimization strategies to improve real-time performance"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter has covered cognitive planning using LLMs to convert natural language commands into ROS 2 action sequences. You've learned about prompt engineering, safety validation, context management, and integration with the ROS 2 ecosystem. The foundation laid here will be essential for the complete VLA integration covered in the next chapter."}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll explore the complete Vision-Language-Action integration, combining voice processing, cognitive planning, and robotic execution into a unified system for autonomous humanoid task execution."})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453(n,e,t){t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);